{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3.2'\n",
    "PDF_LINK = \"pdf/mapreduce-osdi04.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 0}, page_content=\"MapReduce: Simpli\\x02ed Data Processing onLargeClusters\\nJeffreyDean andSanjay Ghema wat\\njeff@google.com, sanjay@google.com\\nGoogle,Inc.\\nAbstract\\nMapReduce isaprogramming model andanassoci-\\nated implementation forprocessing andgenerating large\\ndata sets. Users specify amap function thatprocesses a\\nkey/valuepairtogenerate asetofintermediate key/value\\npairs, andareduce function thatmergesallintermediate\\nvalues associated with thesame intermediate key.Many\\nrealworld tasks areexpressible inthismodel, asshown\\ninthepaper .\\nPrograms written inthisfunctional style areautomati-\\ncally parallelized andexecuted onalargecluster ofcom-\\nmodity machines. Therun-time system takescare ofthe\\ndetails ofpartitioning theinput data, scheduling thepro-\\ngram' sexecution across asetofmachines, handling ma-\\nchine failures, andmanaging therequired inter-machine\\ncommunication. This allowsprogrammers without any\\nexperience with parallel anddistrib uted systems toeas-\\nilyutilize theresources ofalargedistrib uted system.\\nOur implementation ofMapReduce runs onalarge\\ncluster ofcommodity machines andishighly scalable:\\natypical MapReduce computation processes manyter-\\nabytes ofdata onthousands ofmachines. Programmers\\n\\x02ndthesystem easy touse: hundreds ofMapReduce pro-\\ngrams havebeen implemented andupwards ofonethou-\\nsand MapReduce jobs areexecuted onGoogle' sclusters\\neveryday.\\n1Introduction\\nOverthepast \\x02veyears, theauthors andmanyothers at\\nGoogle haveimplemented hundreds ofspecial-purpose\\ncomputations that process largeamounts ofrawdata,\\nsuch ascrawled documents, web request logs, etc., to\\ncompute various kinds ofderiveddata, such asinverted\\nindices, various representations ofthegraph structure\\nofweb documents, summaries ofthenumber ofpages\\ncrawled perhost, thesetofmost frequent queries inagivenday,etc. Most such computations areconceptu-\\nallystraightforw ard. However,theinput data isusually\\nlargeandthecomputations havetobedistrib uted across\\nhundreds orthousands ofmachines inorder to\\x02nish in\\nareasonable amount oftime. Theissues ofhowtopar-\\nallelize thecomputation, distrib utethedata, andhandle\\nfailures conspire toobscure theoriginal simple compu-\\ntation with largeamounts ofcomple xcode todeal with\\nthese issues.\\nAsareaction tothiscomple xity,wedesigned anew\\nabstraction thatallowsustoexpress thesimple computa-\\ntions wewere trying toperform buthides themessy de-\\ntails ofparallelization, fault-tolerance, data distrib ution\\nandload balancing inalibrary .Our abstraction isin-\\nspired bythemap andreduce primiti vespresent inLisp\\nandmanyother functional languages. Werealized that\\nmost ofourcomputations involvedapplying amap op-\\neration toeach logical \\x93record\\x94 inourinput inorder to\\ncompute asetofintermediate key/value pairs, andthen\\napplying areduce operation toallthevalues thatshared\\nthesame key,inorder tocombine thederiveddata ap-\\npropriately .Our useofafunctional model with user-\\nspeci\\x02ed map andreduce operations allowsustoparal-\\nlelize largecomputations easily andtousere-execution\\nastheprimary mechanism forfaulttolerance.\\nThemajor contrib utions ofthisworkareasimple and\\npowerful interf acethatenables automatic parallelization\\nanddistrib ution oflarge-scale computations, combined\\nwith animplementation ofthisinterf acethat achie ves\\nhigh performance onlargeclusters ofcommodity PCs.\\nSection 2describes thebasic programming model and\\ngivesseveralexamples. Section 3describes animple-\\nmentation oftheMapReduce interf acetailored towards\\nourcluster -based computing environment. Section 4de-\\nscribes several re\\x02nements oftheprogramming model\\nthatwehavefound useful. Section 5hasperformance\\nmeasurements ofourimplementation foravariety of\\ntasks. Section 6explores theuseofMapReduce within\\nGoogle including ourexperiences inusing itasthebasis\\nToappear inOSDI 2004 1\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 1}, page_content='forarewrite ofourproduction indexing system. Sec-\\ntion7discusses related andfuture work.\\n2Programming Model\\nThecomputation takesasetofinput key/value pairs, and\\nproduces asetofoutput key/value pairs. The user of\\ntheMapReduce library expresses thecomputation astwo\\nfunctions: Map andReduce .\\nMap,written bytheuser,takesaninput pairandpro-\\nduces asetofintermediate key/value pairs. TheMapRe-\\nduce library groups together allintermediate values asso-\\nciated with thesame intermediate keyIandpasses them\\ntotheReduce function.\\nTheReduce function, alsowritten bytheuser,accepts\\nanintermediate keyIandasetofvalues forthatkey.It\\nmergestogether these values toform apossibly smaller\\nsetofvalues. Typically justzero oroneoutput value is\\nproduced perReduce invocation. The intermediate val-\\nuesaresupplied totheuser\\' sreduce function viaaniter-\\nator.This allowsustohandle lists ofvalues thataretoo\\nlargeto\\x02tinmemory .\\n2.1 Example\\nConsider theproblem ofcounting thenumber ofoc-\\ncurrences ofeach wordinalargecollection ofdocu-\\nments. Theuser would write code similar tothefollo w-\\ningpseudo-code:\\nmap(String key,Stringvalue):\\n//key:document name\\n//value:document contents\\nforeachwordwinvalue:\\nEmitIntermediate(w, \"1\");\\nreduce(String key,Iterator values):\\n//key:aword\\n//values: alistofcounts\\nintresult=0;\\nforeachvinvalues:\\nresult+=ParseInt(v);\\nEmit(AsString(result));\\nThemap function emits each wordplus anassociated\\ncount ofoccurrences (just `1\\'inthissimple example).\\nThereduce function sums together allcounts emitted\\nforaparticular word.\\nInaddition, theuser writes code to\\x02llinamapr educe\\nspeci\\x02cation object with thenames oftheinput andout-\\nput\\x02les, andoptional tuning parameters. Theuser then\\ninvokestheMapReduce function, passing itthespeci\\x02-\\ncation object. Theuser\\' scode islinkedtogether with the\\nMapReduce library (implemented inC++). Appendix A\\ncontains thefullprogram textforthisexample.2.2 Types\\nEventhough theprevious pseudo-code iswritten interms\\nofstring inputs andoutputs, conceptually themap and\\nreduce functions supplied bytheuser haveassociated\\ntypes:\\nmap(k1,v1) !list(k2,v2)\\nreduce(k2,list(v2)) !list(v2)\\nI.e.,theinput keysandvalues aredrawnfrom adifferent\\ndomain than theoutput keysandvalues. Furthermore,\\ntheintermediate keysandvalues arefrom thesame do-\\nmain astheoutput keysandvalues.\\nOur C++ implementation passes strings toandfrom\\ntheuser-de\\x02ned functions andleavesittotheuser code\\ntoconvertbetween strings andappropriate types.\\n2.3 MoreExamples\\nHere areafewsimple examples ofinteresting programs\\nthat canbeeasily expressed asMapReduce computa-\\ntions.\\nDistrib uted Grep: Themap function emits alineifit\\nmatches asupplied pattern. The reduce function isan\\nidentity function thatjustcopies thesupplied intermedi-\\natedata totheoutput.\\nCount ofURL Access Frequency: The map func-\\ntion processes logs ofweb page requests and outputs\\nhURL;1i.The reduce function adds together allvalues\\nforthesame URL andemits ahURL;totalcount i\\npair.\\nReverse Web-Link Graph: Themap function outputs\\nhtarget ;source ipairs foreach link toatarget\\nURL found inapage namedsource .The reduce\\nfunction concatenates thelistofallsource URLs as-\\nsociated with agiventargetURL and emits thepair:\\nhtarget ;list(source )i\\nTerm-V ector perHost: Aterm vector summarizes the\\nmost important words thatoccur inadocument oraset\\nofdocuments asalistofhword;frequency ipairs. The\\nmap function emits ahhostname ;termvector i\\npair foreach input document (where thehostname is\\nextracted from theURL ofthedocument). The re-\\nduce function ispassed allper-document term vectors\\nforagivenhost. Itadds these term vectors together ,\\nthrowing awayinfrequent terms, andthen emits a\\x02nal\\nhhostname ;termvector ipair.\\nToappear inOSDI 2004 2'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 2}, page_content='User\\nProgram\\nMaster(1) fork\\nworker(1) fork\\nworker(1) fork\\n(2)\\nassign\\nmap(2)\\nassign\\nreduce\\nsplit 0\\nsplit 1\\nsplit 2\\nsplit 3\\nsplit 4  \\noutput\\nfile 0    (6) write\\nworker(3) read\\nworker  (4) local write\\n  \\nMap\\nphaseIntermediate files\\n(on local disks)workeroutput\\nfile 1\\nInput\\nfiles(5) remote read\\nReduce\\nphaseOutput\\nfiles\\nFigure 1:Execution overvie w\\nInverted Index: The map function parses each docu-\\nment, andemits asequence ofhword;document IDi\\npairs. The reduce function accepts allpairs foragiven\\nword,sorts thecorresponding document IDsandemits a\\nhword;list(document ID)ipair.Thesetofalloutput\\npairs forms asimple inverted index.Itiseasy toaugment\\nthiscomputation tokeeptrack ofwordpositions.\\nDistrib uted Sort: The map function extracts thekey\\nfrom each record, andemits ahkey;record ipair.The\\nreduce function emits allpairs unchanged. This compu-\\ntation depends onthepartitioning facilities described in\\nSection 4.1andtheordering properties described inSec-\\ntion4.2.\\n3Implementation\\nManydifferent implementations oftheMapReduce in-\\nterfacearepossible. The right choice depends onthe\\nenvironment. Forexample, oneimplementation may be\\nsuitable forasmall shared-memory machine, another for\\nalargeNUMA multi-processor ,andyetanother foran\\nevenlargercollection ofnetw orkedmachines.\\nThis section describes animplementation targeted\\ntothecomputing environment inwide useatGoogle:largeclusters ofcommodity PCsconnected together with\\nswitched Ethernet [4].Inourenvironment:\\n(1)Machines aretypically dual-processor x86processors\\nrunning Linux, with 2-4GBofmemory permachine.\\n(2)Commodity netw orking hardw areisused \\x96typically\\neither 100 megabits/second or1gigabit/second atthe\\nmachine level,butaveraging considerably lessinover-\\nallbisection bandwidth.\\n(3)Acluster consists ofhundreds orthousands ofma-\\nchines, andtherefore machine failures arecommon.\\n(4)Storage isprovided byinexpensi veIDE disks at-\\ntached directly toindividual machines. Adistrib uted \\x02le\\nsystem [8]developed in-house isused tomanage thedata\\nstored onthese disks. The\\x02lesystem uses replication to\\nprovide availability andreliability ontopofunreliable\\nhardw are.\\n(5)Users submit jobs toascheduling system. Each job\\nconsists ofasetoftasks, andismapped bythescheduler\\ntoasetofavailable machines within acluster .\\n3.1 Execution Overview\\nThe Map invocations aredistrib uted across multiple\\nmachines byautomatically partitioning theinput data\\nToappear inOSDI 2004 3'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 3}, page_content=\"into asetofMsplits .The input splits canbepro-\\ncessed inparallel bydifferent machines. Reduce invoca-\\ntions aredistrib uted bypartitioning theintermediate key\\nspace intoRpieces using apartitioning function (e.g.,\\nhash(key)modR).Thenumber ofpartitions (R)and\\nthepartitioning function arespeci\\x02ed bytheuser.\\nFigure 1showstheoverall \\x03owofaMapReduce op-\\neration inourimplementation. When theuser program\\ncalls theMapReduce function, thefollo wing sequence\\nofactions occurs (thenumbered labels inFigure 1corre-\\nspond tothenumbers inthelistbelow):\\n1.The MapReduce library intheuser program \\x02rst\\nsplits theinput \\x02les intoMpieces oftypically 16\\nmegabytes to64megabytes (MB) perpiece (con-\\ntrollable bytheuser viaanoptional parameter). It\\nthen starts upmanycopies oftheprogram onaclus-\\nterofmachines.\\n2.One ofthecopies oftheprogram isspecial \\x96the\\nmaster .Therestareworkersthatareassigned work\\nbythemaster .There areMmap tasks andRreduce\\ntasks toassign. Themaster picks idleworkersand\\nassigns each oneamap taskorareduce task.\\n3.Aworkerwho isassigned amap task reads the\\ncontents ofthecorresponding input split. Itparses\\nkey/valuepairs outoftheinput data andpasses each\\npairtotheuser-de\\x02ned Map function. Theinterme-\\ndiate key/value pairs produced bytheMap function\\narebuffered inmemory .\\n4.Periodically ,thebuffered pairs arewritten tolocal\\ndisk, partitioned intoRregions bythepartitioning\\nfunction. The locations ofthese buffered pairs on\\nthelocal disk arepassed back tothemaster ,who\\nisresponsible forforw arding these locations tothe\\nreduce workers.\\n5.When areduce workerisnoti\\x02ed bythemaster\\nabout these locations, ituses remote procedure calls\\ntoread thebuffered data from thelocal disks ofthe\\nmap workers.When areduce workerhasread allin-\\ntermediate data, itsorts itbytheintermediate keys\\nsothatalloccurrences ofthesame keyaregrouped\\ntogether .The sorting isneeded because typically\\nmanydifferent keysmap tothesame reduce task. If\\ntheamount ofintermediate data istoolargeto\\x02tin\\nmemory ,anexternal sortisused.\\n6.Thereduce workeriterates overthesorted interme-\\ndiate data andforeach unique intermediate keyen-\\ncountered, itpasses thekeyandthecorresponding\\nsetofintermediate values totheuser' sReduce func-\\ntion. Theoutput oftheReduce function isappended\\ntoa\\x02nal output \\x02leforthisreduce partition.7.When allmap tasks and reduce tasks havebeen\\ncompleted, themaster wakesuptheuser program.\\nAtthispoint, theMapReduce callintheuser pro-\\ngram returns back totheuser code.\\nAfter successful completion, theoutput ofthemapre-\\nduce execution isavailable intheRoutput \\x02les (one per\\nreduce task, with \\x02lenames asspeci\\x02ed bytheuser).\\nTypically ,users donotneed tocombine theseRoutput\\n\\x02les intoone\\x02le\\x96theyoften pass these \\x02les asinput to\\nanother MapReduce call, orusethem from another dis-\\ntributed application thatisable todeal with input thatis\\npartitioned intomultiple \\x02les.\\n3.2 Master Data Structur es\\nThemaster keeps severaldata structures. Foreach map\\ntaskandreduce task, itstores thestate (idle,in-pr ogress,\\norcompleted ),andtheidentity oftheworkermachine\\n(fornon-idle tasks).\\nThemaster istheconduit through which thelocation\\nofintermediate \\x02leregions ispropagated from map tasks\\ntoreduce tasks. Therefore, foreach completed map task,\\nthemaster stores thelocations andsizes oftheRinter-\\nmediate \\x02leregions produced bythemap task. Updates\\ntothislocation andsizeinformation arerecei vedasmap\\ntasks arecompleted. The information ispushed incre-\\nmentally toworkersthathavein-pr ogressreduce tasks.\\n3.3 Fault Tolerance\\nSince theMapReduce library isdesigned tohelp process\\nverylargeamounts ofdata using hundreds orthousands\\nofmachines, thelibrary must tolerate machine failures\\ngracefully .\\nWorkerFailur e\\nThe master pings everyworkerperiodically .Ifnore-\\nsponse isrecei vedfrom aworkerinacertain amount of\\ntime, themaster marks theworkerasfailed. Anymap\\ntasks completed bytheworkerarereset back totheir ini-\\ntialidlestate, andtherefore become eligible forschedul-\\ningonother workers.Similarly ,anymap task orreduce\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 3}, page_content='tasks completed bytheworkerarereset back totheir ini-\\ntialidlestate, andtherefore become eligible forschedul-\\ningonother workers.Similarly ,anymap task orreduce\\ntask inprogress onafailed workerisalso reset toidle\\nandbecomes eligible forrescheduling.\\nCompleted map tasks arere-executed onafailure be-\\ncause their output isstored onthelocal disk(s) ofthe\\nfailed machine andistherefore inaccessible. Completed\\nreduce tasks donotneed tobere-executed since their\\noutput isstored inaglobal \\x02lesystem.\\nWhen amap task isexecuted \\x02rst byworkerAand\\nthen later executed byworkerB(because Afailed), all\\nToappear inOSDI 2004 4'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 4}, page_content=\"workersexecuting reduce tasks arenoti\\x02ed ofthere-\\nexecution. Anyreduce task thathasnotalready read the\\ndata from workerAwillread thedata from workerB.\\nMapReduce isresilient tolarge-scale workerfailures.\\nForexample, during oneMapReduce operation, netw ork\\nmaintenance onarunning cluster wascausing groups of\\n80machines atatime tobecome unreachable forsev-\\neralminutes. TheMapReduce master simply re-executed\\ntheworkdone bytheunreachable workermachines, and\\ncontinued tomakeforw ardprogress, eventually complet-\\ningtheMapReduce operation.\\nMaster Failur e\\nItiseasy tomakethemaster write periodic checkpoints\\nofthemaster data structures described above.Ifthemas-\\ntertask dies, anewcopycanbestarted from thelast\\ncheckpointed state. However,giventhatthere isonly a\\nsingle master ,itsfailure isunlik ely; therefore ourcur-\\nrentimplementation aborts theMapReduce computation\\nifthemaster fails. Clients cancheck forthiscondition\\nandretry theMapReduce operation iftheydesire.\\nSemantics inthePresence ofFailur es\\nWhen theuser-supplied map andreduce operators arede-\\nterministic functions oftheir input values, ourdistrib uted\\nimplementation produces thesame output aswould have\\nbeen produced byanon-f aulting sequential execution of\\ntheentire program.\\nWerely onatomic commits ofmap andreduce task\\noutputs toachie vethisproperty .Each in-progress task\\nwrites itsoutput toprivatetemporary \\x02les. Areduce task\\nproduces onesuch \\x02le,andamap task produces Rsuch\\n\\x02les (one perreduce task). When amap task completes,\\ntheworkersends amessage tothemaster andincludes\\nthenames oftheRtemporary \\x02les inthemessage. If\\nthemaster recei vesacompletion message foranalready\\ncompleted map task, itignores themessage. Otherwise,\\nitrecords thenames ofR\\x02les inamaster data structure.\\nWhen areduce task completes, thereduce worker\\natomically renames itstemporary output \\x02letothe\\x02nal\\noutput \\x02le.Ifthesame reduce taskisexecuted onmulti-\\nplemachines, multiple rename calls willbeexecuted for\\nthesame \\x02nal output \\x02le.Werelyontheatomic rename\\noperation provided bytheunderlying \\x02lesystem toguar-\\nantee thatthe\\x02nal \\x02lesystem state contains justthedata\\nproduced byoneexecution ofthereduce task.\\nThevastmajority ofourmap andreduce operators are\\ndeterministic, andthefactthatoursemantics areequiv-\\nalent toasequential execution inthiscase makesitveryeasy forprogrammers toreason about their program' sbe-\\nhavior.When themap and/or reduce operators arenon-\\ndeterministic, weprovide weak erbutstillreasonable se-\\nmantics. Inthepresence ofnon-deterministic operators,\\ntheoutput ofaparticular reduce taskR1isequivalent to\\ntheoutput forR1produced byasequential execution of\\nthenon-deterministic program. However,theoutput for\\nadifferent reduce taskR2may correspond totheoutput\\nforR2produced byadifferent sequential execution of\\nthenon-deterministic program.\\nConsider map taskMandreduce tasksR1andR2.\\nLete(Ri)betheexecution ofRithatcommitted (there\\nisexactly onesuch execution). The weak ersemantics\\narise because e(R1)may haveread theoutput produced\\nbyoneexecution ofMande(R2)may haveread the\\noutput produced byadifferent execution ofM.\\n3.4 Locality\\nNetw orkbandwidth isarelati velyscarce resource inour\\ncomputing environment. Weconserv enetw orkband-\\nwidth bytaking advantage ofthefactthattheinput data\\n(managed byGFS [8])isstored onthelocal disks ofthe\\nmachines that makeupourcluster .GFS divides each\\n\\x02leinto64MBblocks, andstores severalcopies ofeach\\nblock (typically 3copies) ondifferent machines. The\\nMapReduce master takesthelocation information ofthe\\ninput \\x02les into account andattempts toschedule amap\\ntask onamachine thatcontains areplica ofthecorre-\\nsponding input data. Failing that, itattempts toschedule\\namap tasknear areplica ofthattask' sinput data (e.g., on\\naworkermachine thatisonthesame netw orkswitch as\\nthemachine containing thedata). When running large\\nMapReduce operations onasigni\\x02cant fraction ofthe\\nworkersinacluster ,most input data isread locally and\\nconsumes nonetw orkbandwidth.\\n3.5 TaskGranularity\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 4}, page_content='MapReduce operations onasigni\\x02cant fraction ofthe\\nworkersinacluster ,most input data isread locally and\\nconsumes nonetw orkbandwidth.\\n3.5 TaskGranularity\\nWesubdi vide themap phase intoMpieces andthere-\\nduce phase intoRpieces, asdescribed above.Ideally ,M\\nandRshould bemuch largerthan thenumber ofworker\\nmachines. Having each workerperform manydifferent\\ntasks impro vesdynamic load balancing, andalso speeds\\nuprecoverywhen aworkerfails: themanymap tasks\\nithascompleted canbespread outacross alltheother\\nworkermachines.\\nThere arepractical bounds onhowlargeMandRcan\\nbeinourimplementation, since themaster must make\\nO(M+R)scheduling decisions andkeepsO(M\\x03R)\\nstate inmemory asdescribed above.(The constant fac-\\ntorsformemory usage aresmall however:theO(M\\x03R)\\npiece ofthestate consists ofapproximately onebyte of\\ndata permap task/reduce taskpair.)\\nToappear inOSDI 2004 5'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 5}, page_content='Furthermore, Risoften constrained byusers because\\ntheoutput ofeach reduce task ends upinaseparate out-\\nput\\x02le. Inpractice, wetend tochoose Msothateach\\nindividual taskisroughly 16MBto64MBofinput data\\n(sothatthelocality optimization described aboveismost\\neffective),andwemakeRasmall multiple ofthenum-\\nberofworkermachines weexpect touse. Weoften per-\\nform MapReduce computations withM=200;000and\\nR=5;000,using 2,000 workermachines.\\n3.6 Backup Tasks\\nOne ofthecommon causes thatlengthens thetotal time\\ntakenforaMapReduce operation isa\\x93straggler\\x94: ama-\\nchine thattakesanunusually long time tocomplete one\\nofthelastfewmap orreduce tasks inthecomputation.\\nStragglers canarise forawhole host ofreasons. Forex-\\nample, amachine with abaddisk may experience fre-\\nquent correctable errors thatslowitsread performance\\nfrom 30MB/s to1MB/s. The cluster scheduling sys-\\ntem may havescheduled other tasks onthemachine,\\ncausing ittoexecute theMapReduce code more slowly\\nduetocompetition forCPU, memory ,local disk, ornet-\\nworkbandwidth. Arecent problem weexperienced was\\nabuginmachine initialization code thatcaused proces-\\nsorcaches tobedisabled: computations onaffected ma-\\nchines slowed downbyoverafactor ofonehundred.\\nWehaveageneral mechanism toalleviate theprob-\\nlemofstragglers. When aMapReduce operation isclose\\ntocompletion, themaster schedules backup executions\\noftheremaining in-pr ogresstasks. The task ismark ed\\nascompleted whene vereither theprimary orthebackup\\nexecution completes. Wehavetuned thismechanism so\\nthat ittypically increases thecomputational resources\\nused bytheoperation bynomore than afewpercent.\\nWehavefound that thissigni\\x02cantly reduces thetime\\ntocomplete largeMapReduce operations. Asanexam-\\nple,thesortprogram described inSection 5.3takes44%\\nlonger tocomplete when thebackup task mechanism is\\ndisabled.\\n4Re\\x02nements\\nAlthough thebasic functionality provided bysimply\\nwriting Map andReduce functions issuf\\x02cient formost\\nneeds, wehavefound afewextensions useful. These are\\ndescribed inthissection.\\n4.1 Partitioning Function\\nThe users ofMapReduce specify thenumber ofreduce\\ntasks/output \\x02les thattheydesire (R).Data gets parti-\\ntioned across these tasks using apartitioning function ontheintermediate key.Adefaultpartitioning function is\\nprovided thatuses hashing (e.g. \\x93hash(key)modR\\x94).\\nThis tends toresult infairly well-balanced partitions. In\\nsome cases, however,itisuseful topartition data by\\nsome other function ofthekey.Forexample, sometimes\\ntheoutput keysareURLs, andwewantallentries fora\\nsingle host toendupinthesame output \\x02le. Tosupport\\nsituations likethis, theuser oftheMapReduce library\\ncanprovide aspecial partitioning function. Forexample,\\nusing \\x93hash(Hostname (urlkey))modR\\x94asthepar-\\ntitioning function causes allURLs from thesame host to\\nendupinthesame output \\x02le.\\n4.2 Ordering Guarantees\\nWeguarantee thatwithin agivenpartition, theinterme-\\ndiate key/value pairs areprocessed inincreasing keyor-\\nder.This ordering guarantee makesiteasy togenerate\\nasorted output \\x02leperpartition, which isuseful when\\ntheoutput \\x02leformat needs tosupport ef\\x02cient random\\naccess lookups bykey,orusers oftheoutput \\x02nditcon-\\nvenient tohavethedata sorted.\\n4.3 Combiner Function\\nInsome cases, there issigni\\x02cant repetition intheinter-\\nmediate keysproduced byeach map task, andtheuser-\\nspeci\\x02ed Reduce function iscommutati veandassocia-\\ntive.Agood example ofthisisthewordcounting exam-\\npleinSection 2.1.Since wordfrequencies tend tofollo w\\naZipf distrib ution, each map taskwillproduce hundreds\\northousands ofrecords oftheform<the,1>.Allof\\nthese counts willbesentoverthenetw orktoasingle re-\\nduce taskandthen added together bytheReduce function\\ntoproduce onenumber .Weallowtheuser tospecify an\\noptional Combiner function thatdoes partial merging of\\nthisdata before itissentoverthenetw ork.\\nThe Combiner function isexecuted oneach machine\\nthatperforms amap task. Typically thesame code isused\\ntoimplement both thecombiner andthereduce func-\\ntions. Theonly difference between areduce function and'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 5}, page_content='thatperforms amap task. Typically thesame code isused\\ntoimplement both thecombiner andthereduce func-\\ntions. Theonly difference between areduce function and\\nacombiner function ishowtheMapReduce library han-\\ndles theoutput ofthefunction. The output ofareduce\\nfunction iswritten tothe\\x02nal output \\x02le. Theoutput of\\nacombiner function iswritten toanintermediate \\x02lethat\\nwillbesenttoareduce task.\\nPartial combining signi\\x02cantly speeds upcertain\\nclasses ofMapReduce operations. Appendix Acontains\\nanexample thatuses acombiner .\\n4.4 Input andOutput Types\\nTheMapReduce library provides support forreading in-\\nputdata inseveraldifferent formats. Forexample, \\x93text\\x94\\nToappear inOSDI 2004 6'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 6}, page_content=\"mode input treats each lineasakey/value pair: thekey\\nistheoffsetinthe\\x02leandthevalue isthecontents of\\ntheline. Another common supported format stores a\\nsequence ofkey/value pairs sorted bykey.Each input\\ntype implementation knowshowtosplit itself intomean-\\ningful ranges forprocessing asseparate map tasks (e.g.\\ntextmode' srange splitting ensures thatrange splits oc-\\ncuronly atlineboundaries). Users canaddsupport fora\\nnewinput type byproviding animplementation ofasim-\\nplereader interf ace,though most users justuseoneofa\\nsmall number ofprede\\x02ned input types.\\nAreader does notnecessarily need toprovide data\\nread from a\\x02le.Forexample, itiseasy tode\\x02ne areader\\nthatreads records from adatabase, orfrom data struc-\\ntures mapped inmemory .\\nInasimilar fashion, wesupport asetofoutput types\\nforproducing data indifferent formats anditiseasy for\\nuser code toaddsupport fornewoutput types.\\n4.5 Side-effects\\nInsome cases, users ofMapReduce havefound itcon-\\nvenient toproduce auxiliary \\x02les asadditional outputs\\nfrom their map and/or reduce operators. Werelyonthe\\napplication writer tomakesuch side-ef fects atomic and\\nidempotent. Typically theapplication writes toatempo-\\nrary\\x02leandatomically renames this\\x02leonce ithasbeen\\nfully generated.\\nWedonotprovide support foratomic two-phase com-\\nmits ofmultiple output \\x02les produced byasingle task.\\nTherefore, tasks thatproduce multiple output \\x02les with\\ncross-\\x02le consistenc yrequirements should bedetermin-\\nistic. This restriction hasneverbeen anissue inpractice.\\n4.6 Skipping Bad Records\\nSometimes there arebugsinusercode thatcause theMap\\norReduce functions tocrash deterministically oncertain\\nrecords. Such bugspreventaMapReduce operation from\\ncompleting. Theusual course ofaction isto\\x02xthebug,\\nbutsometimes thisisnotfeasible; perhaps thebugisin\\nathird-party library forwhich source code isunavail-\\nable. Also, sometimes itisacceptable toignore afew\\nrecords, forexample when doing statistical analysis on\\nalargedata set.Weprovide anoptional mode ofexecu-\\ntionwhere theMapReduce library detects which records\\ncause deterministic crashes andskips these records inor-\\ndertomakeforw ardprogress.\\nEach workerprocess installs asignal handler that\\ncatches segmentation violations andbuserrors. Before\\ninvoking auser Map orReduce operation, theMapRe-\\nduce library stores thesequence number oftheargument\\ninaglobal variable. Iftheuser code generates asignal,thesignal handler sends a\\x93last gasp\\x94 UDP pack etthat\\ncontains thesequence number totheMapReduce mas-\\nter.When themaster hasseen more than onefailure on\\naparticular record, itindicates thattherecord should be\\nskipped when itissues thenextre-execution ofthecorre-\\nsponding Map orReduce task.\\n4.7 Local Execution\\nDebugging problems inMap orReduce functions canbe\\ntricky,since theactual computation happens inadis-\\ntributed system, often onseveral thousand machines,\\nwith workassignment decisions made dynamically by\\nthemaster .Tohelp facilitate debugging, pro\\x02ling, and\\nsmall-scale testing, wehavedeveloped analternati veim-\\nplementation oftheMapReduce library thatsequentially\\nexecutes alloftheworkforaMapReduce operation on\\nthelocal machine. Controls areprovided totheuser so\\nthat thecomputation canbelimited toparticular map\\ntasks. Users invoketheir program with aspecial \\x03agand\\ncanthen easily useanydebugging ortesting tools they\\n\\x02nduseful (e.g.gdb).\\n4.8 Status Information\\nThe master runs aninternal HTTP serverandexports\\nasetofstatus pages forhuman consumption. The sta-\\ntuspages showtheprogress ofthecomputation, such as\\nhowmanytasks havebeen completed, howmanyarein\\nprogress, bytes ofinput, bytes ofintermediate data, bytes\\nofoutput, processing rates, etc. Thepages also contain\\nlinks tothestandard error andstandard output \\x02les gen-\\nerated byeach task. The user canusethisdata topre-\\ndicthowlong thecomputation willtake,andwhether or\\nnotmore resources should beadded tothecomputation.\\nThese pages canalsobeused to\\x02gure outwhen thecom-\\nputation ismuch slowerthan expected.\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 6}, page_content='dicthowlong thecomputation willtake,andwhether or\\nnotmore resources should beadded tothecomputation.\\nThese pages canalsobeused to\\x02gure outwhen thecom-\\nputation ismuch slowerthan expected.\\nInaddition, thetop-le velstatus page showswhich\\nworkershavefailed, andwhich map andreduce tasks\\ntheywere processing when theyfailed. This informa-\\ntion isuseful when attempting todiagnose bugsinthe\\nuser code.\\n4.9 Counters\\nThe MapReduce library provides acounter facility to\\ncount occurrences ofvarious events. Forexample, user\\ncode may wanttocount total number ofwords processed\\northenumber ofGerman documents indexed,etc.\\nTousethisfacility ,user code creates anamed counter\\nobject andthen increments thecounter appropriately in\\ntheMap and/or Reduce function. Forexample:\\nToappear inOSDI 2004 7'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 7}, page_content='Counter* uppercase;\\nuppercase =GetCounter(\"uppercase\");\\nmap(String name,Stringcontents):\\nforeachwordwincontents:\\nif(IsCapitalized(w)):\\nuppercase->Increment();\\nEmitIntermediate(w, \"1\");\\nThe counter values from individual workermachines\\nareperiodically propagated tothemaster (piggyback ed\\nontheping response). Themaster aggre gates thecounter\\nvalues from successful map andreduce tasks andreturns\\nthem totheuser code when theMapReduce operation\\niscompleted. The current counter values arealso dis-\\nplayed onthemaster status page sothat ahuman can\\nwatch theprogress ofthelivecomputation. When aggre-\\ngating counter values, themaster eliminates theeffects of\\nduplicate executions ofthesame map orreduce task to\\navoiddouble counting. (Duplicate executions canarise\\nfrom ouruseofbackup tasks andfrom re-execution of\\ntasks duetofailures.)\\nSome counter values areautomatically maintained\\nbytheMapReduce library ,such asthenumber ofin-\\nputkey/value pairs processed andthenumber ofoutput\\nkey/value pairs produced.\\nUsers havefound thecounter facility useful forsan-\\nitychecking thebeha viorofMapReduce operations. For\\nexample, insome MapReduce operations, theuser code\\nmay wanttoensure that thenumber ofoutput pairs\\nproduced exactly equals thenumber ofinput pairs pro-\\ncessed, orthatthefraction ofGerman documents pro-\\ncessed iswithin some tolerable fraction ofthetotal num-\\nberofdocuments processed.\\n5Performance\\nInthissection wemeasure theperformance ofMapRe-\\nduce ontwocomputations running onalargecluster of\\nmachines. One computation searches through approxi-\\nmately oneterabyte ofdata looking foraparticular pat-\\ntern. Theother computation sorts approximately oneter-\\nabyte ofdata.\\nThese twoprograms arerepresentati veofalargesub-\\nsetoftherealprograms written byusers ofMapReduce \\x96\\noneclass ofprograms shuf\\x03esdata from onerepresenta-\\ntiontoanother ,andanother class extracts asmall amount\\nofinteresting data from alargedata set.\\n5.1 Cluster Con\\x02guration\\nAlloftheprograms were executed onacluster that\\nconsisted ofapproximately 1800 machines. Each ma-\\nchine hadtwo2GHz Intel Xeon processors with Hyper -\\nThreading enabled, 4GB ofmemory ,two160GB IDE20 40 60 80 100\\nSeconds0100002000030000Input (MB/s)\\nFigure 2:Data transfer rateovertime\\ndisks, andagigabit Ethernet link. The machines were\\narranged inatwo-leveltree-shaped switched netw ork\\nwith approximately 100-200 Gbps ofaggre gate band-\\nwidth available attheroot. Allofthemachines were\\ninthesame hosting facility andtherefore theround-trip\\ntime between anypairofmachines waslessthan amil-\\nlisecond.\\nOutofthe4GB ofmemory ,approximately 1-1.5GB\\nwasreserv edbyother tasks running onthecluster .The\\nprograms were executed onaweek endafternoon, when\\ntheCPUs, disks, andnetw orkwere mostly idle.\\n5.2 Grep\\nThegrepprogram scans through 1010100-byte records,\\nsearching forarelati velyrarethree-character pattern (the\\npattern occurs in92,337 records). Theinput issplit into\\napproximately 64MB pieces (M=15000 ),andtheen-\\ntireoutput isplaced inone\\x02le(R=1).\\nFigure 2showstheprogress ofthecomputation over\\ntime. TheY-axis showstherateatwhich theinput data is\\nscanned. Therategradually picks upasmore machines\\nareassigned tothisMapReduce computation, andpeaks\\natover30GB/s when 1764 workershavebeen assigned.\\nAsthemap tasks \\x02nish, theratestarts dropping andhits\\nzero about 80seconds intothecomputation. Theentire\\ncomputation takesapproximately 150seconds from start\\nto\\x02nish. This includes about aminute ofstartup over-\\nhead. Theoverhead isduetothepropagation ofthepro-\\ngram toallworkermachines, anddelays interacting with\\nGFS toopen thesetof1000 input \\x02les andtogetthe\\ninformation needed forthelocality optimization.\\n5.3 Sort\\nThesort program sorts1010100-byte records (approxi-\\nmately 1terabyte ofdata). This program ismodeled after\\ntheTeraSort benchmark [10].\\nThe sorting program consists oflessthan 50lines of\\nuser code. Athree-line Map function extracts a10-byte\\nsorting keyfrom atextlineandemits thekeyandthe\\nToappear inOSDI 2004 8'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 8}, page_content='500 100005000100001500020000Input (MB/s)\\n500 100005000100001500020000Shuffle (MB/s)\\n500 1000\\nSeconds05000100001500020000Output (MB/s)Done\\n(a)Normal execution500 100005000100001500020000Input (MB/s)\\n500 100005000100001500020000Shuffle (MB/s)\\n500 1000\\nSeconds05000100001500020000Output (MB/s)Done\\n(b)Nobackup tasks500 100005000100001500020000Input (MB/s)\\n500 100005000100001500020000Shuffle (MB/s)\\n500 1000\\nSeconds05000100001500020000Output (MB/s)Done\\n(c)200tasks killed\\nFigure 3:Data transfer rates overtime fordifferent executions ofthesortprogram\\noriginal textlineastheintermediate key/value pair.We\\nused abuilt-in Identity function astheReduce operator .\\nThis functions passes theintermediate key/valuepairun-\\nchanged astheoutput key/value pair.The \\x02nal sorted\\noutput iswritten toasetof2-wayreplicated GFS \\x02les\\n(i.e., 2terabytes arewritten astheoutput oftheprogram).\\nAsbefore, theinput data issplit into 64MB pieces\\n(M=15000 ).Wepartition thesorted output into4000\\n\\x02les (R=4000 ).Thepartitioning function uses theini-\\ntialbytes ofthekeytosegregate itintooneofRpieces.\\nOurpartitioning function forthisbenchmark hasbuilt-\\ninknowledge ofthedistrib ution ofkeys.Inageneral\\nsorting program, wewould addapre-pass MapReduce\\noperation that would collect asample ofthekeysand\\nusethedistrib ution ofthesampled keystocompute split-\\npoints forthe\\x02nal sorting pass.\\nFigure 3(a)showstheprogress ofanormal execution\\nofthesortprogram. The top-left graph showstherate\\natwhich input isread. Theratepeaks atabout 13GB/s\\nanddies offfairly quickly since allmap tasks \\x02nish be-\\nfore 200seconds haveelapsed. Note thattheinput rate\\nislessthan forgrep.This isbecause thesortmap tasks\\nspend about halftheir time andI/Obandwidth writing in-\\ntermediate output totheir local disks. Thecorresponding\\nintermediate output forgrep hadnegligible size.\\nThe middle-left graph showstherate atwhich data\\nissent overthenetw orkfrom themap tasks tothere-\\nduce tasks. This shuf\\x03ing starts assoon asthe\\x02rst\\nmap task completes. The \\x02rst hump inthegraph isforthe\\x02rst batch ofapproximately 1700 reduce tasks (the\\nentire MapReduce wasassigned about 1700 machines,\\nandeach machine executes atmost onereduce task ata\\ntime). Roughly 300seconds intothecomputation, some\\nofthese \\x02rst batch ofreduce tasks \\x02nish andwestart\\nshuf\\x03ing data fortheremaining reduce tasks. Allofthe\\nshuf\\x03ing isdone about 600seconds intothecomputation.\\nThebottom-left graph showstherateatwhich sorted\\ndata iswritten tothe\\x02nal output \\x02les bythereduce tasks.\\nThere isadelay between theendofthe\\x02rstshuf\\x03ing pe-\\nriod andthestart ofthewriting period because thema-\\nchines arebusysorting theintermediate data. Thewrites\\ncontinue atarateofabout 2-4GB/s forawhile. Allof\\nthewrites \\x02nish about 850seconds intothecomputation.\\nIncluding startup overhead, theentire computation takes\\n891seconds. This issimilar tothecurrent best reported\\nresult of1057 seconds fortheTeraSort benchmark [18].\\nAfewthings tonote: theinput rateishigher than the\\nshuf\\x03erateandtheoutput ratebecause ofourlocality\\noptimization \\x96most data isread from alocal disk and\\nbypasses ourrelati velybandwidth constrained netw ork.\\nThe shuf\\x03erate ishigher than theoutput rate because\\ntheoutput phase writes twocopies ofthesorted data (we\\nmaketworeplicas oftheoutput forreliability andavail-\\nability reasons). Wewrite tworeplicas because thatis\\nthemechanism forreliability andavailability provided\\nbyourunderlying \\x02lesystem. Netw orkbandwidth re-\\nquirements forwriting data would bereduced iftheun-\\nderlying \\x02lesystem used erasure coding [14]rather than\\nreplication.\\nToappear inOSDI 2004 9'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 9}, page_content=\"5.4 Effect ofBackup Tasks\\nInFigure 3(b),weshowanexecution ofthesortpro-\\ngram with backup tasks disabled. Theexecution \\x03owis\\nsimilar tothatshowninFigure 3(a),except thatthere is\\naverylong tailwhere hardly anywrite activity occurs.\\nAfter 960seconds, allexcept 5ofthereduce tasks are\\ncompleted. Howeverthese lastfewstragglers don't\\x02n-\\nishuntil 300seconds later.Theentire computation takes\\n1283 seconds, anincrease of44% inelapsed time.\\n5.5 Machine Failur es\\nInFigure 3(c),weshowanexecution ofthesortprogram\\nwhere weintentionally killed 200 outof1746 worker\\nprocesses several minutes into thecomputation. The\\nunderlying cluster scheduler immediately restarted new\\nworkerprocesses onthese machines (since only thepro-\\ncesses were killed, themachines were stillfunctioning\\nproperly).\\nThe workerdeaths showupasanegativeinput rate\\nsince some previously completed map workdisappears\\n(since thecorresponding map workerswere killed) and\\nneeds toberedone. There-execution ofthismap work\\nhappens relati velyquickly .The entire computation \\x02n-\\nishes in933seconds including startup overhead (just an\\nincrease of5%overthenormal execution time).\\n6Experience\\nWewrote the\\x02rst version oftheMapReduce library in\\nFebruary of2003, andmade signi\\x02cant enhancements to\\nitinAugust of2003, including thelocality optimization,\\ndynamic load balancing oftask execution across worker\\nmachines, etc.Since thattime, wehavebeen pleasantly\\nsurprised athowbroadly applicable theMapReduce li-\\nbrary hasbeen forthekinds ofproblems weworkon.\\nIthasbeen used across awide range ofdomains within\\nGoogle, including:\\n\\x0flarge-scale machine learning problems,\\n\\x0fclustering problems fortheGoogle Newsand\\nFroogle products,\\n\\x0fextraction ofdataused toproduce reports ofpopular\\nqueries (e.g. Google Zeitgeist),\\n\\x0fextraction ofproperties ofweb pages fornewexper-\\niments andproducts (e.g. extraction ofgeographi-\\ncallocations from alargecorpus ofweb pages for\\nlocalized search), and\\n\\x0flarge-scale graph computations.2003/032003/062003/092003/122004/032004/062004/0902004006008001000Number of instances in source tree\\nFigure 4:MapReduce instances overtime\\nNumber ofjobs 29,423\\nAverage jobcompletion time 634secs\\nMachine days used 79,186 days\\nInput data read 3,288 TB\\nIntermediate data produced 758TB\\nOutput data written 193TB\\nAverage workermachines perjob 157\\nAverage workerdeaths perjob 1.2\\nAverage map tasks perjob 3,351\\nAverage reduce tasks perjob 55\\nUnique map implementations 395\\nUnique reduce implementations 269\\nUnique map/r educe combinations 426\\nTable 1:MapReduce jobs runinAugust 2004\\nFigure 4showsthesigni\\x02cant growthinthenumber of\\nseparate MapReduce programs check edintoourprimary\\nsource code management system overtime, from 0in\\nearly 2003 toalmost 900 separate instances asoflate\\nSeptember 2004. MapReduce hasbeen sosuccessful be-\\ncause itmakesitpossible towrite asimple program and\\nrunitef\\x02ciently onathousand machines inthecourse\\nofhalfanhour,greatly speeding upthedevelopment and\\nprototyping cycle. Furthermore, itallowsprogrammers\\nwho havenoexperience with distrib uted and/or parallel\\nsystems toexploit largeamounts ofresources easily .\\nAttheendofeach job, theMapReduce library logs\\nstatistics about thecomputational resources used bythe\\njob.InTable 1,weshowsome statistics forasubset of\\nMapReduce jobs runatGoogle inAugust 2004.\\n6.1 Large-Scale Indexing\\nOne ofourmost signi\\x02cant uses ofMapReduce todate\\nhasbeen acomplete rewrite oftheproduction index-\\nToappear inOSDI 2004 10\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 10}, page_content='ingsystem thatproduces thedata structures used forthe\\nGoogle web search service. The indexing system takes\\nasinput alargesetofdocuments thathavebeen retrie ved\\nbyourcrawling system, stored asasetofGFS \\x02les. The\\nrawcontents forthese documents aremore than 20ter-\\nabytes ofdata. Theindexing process runs asasequence\\nof\\x02vetotenMapReduce operations. Using MapReduce\\n(instead ofthead-hoc distrib uted passes intheprior ver-\\nsion oftheindexing system) hasprovided severalbene-\\n\\x02ts:\\n\\x0fTheindexing code issimpler ,smaller ,andeasier to\\nunderstand, because thecode thatdeals with fault\\ntolerance, distrib ution andparallelization ishidden\\nwithin theMapReduce library .Forexample, the\\nsizeofonephase ofthecomputation dropped from\\napproximately 3800 lines ofC++ code toapprox-\\nimately 700 lines when expressed using MapRe-\\nduce.\\n\\x0fTheperformance oftheMapReduce library isgood\\nenough that wecankeepconceptually unrelated\\ncomputations separate, instead ofmixing them to-\\ngether toavoidextra passes overthedata. This\\nmakesiteasy tochange theindexing process. For\\nexample, one change that took afewmonths to\\nmakeinouroldindexing system took only afew\\ndays toimplement inthenewsystem.\\n\\x0fThe indexing process hasbecome much easier to\\noperate, because most oftheproblems caused by\\nmachine failures, slowmachines, andnetw orking\\nhiccups aredealt with automatically bytheMapRe-\\nduce library without operator interv ention. Further -\\nmore, itiseasy toimpro vetheperformance ofthe\\nindexing process byadding newmachines tothein-\\ndexing cluster .\\n7Related Work\\nManysystems haveprovided restricted programming\\nmodels andused therestrictions toparallelize thecom-\\nputation automatically .Forexample, anassociati vefunc-\\ntioncanbecomputed overallpre\\x02x esofanNelement\\narray inlogNtime onNprocessors using parallel pre\\x02x\\ncomputations [6,9,13].MapReduce canbeconsidered\\nasimpli\\x02cation anddistillation ofsome ofthese models\\nbased onourexperience with largereal-w orld compu-\\ntations. More signi\\x02cantly ,weprovide afault-tolerant\\nimplementation that scales tothousands ofprocessors.\\nIncontrast, most oftheparallel processing systems have\\nonly been implemented onsmaller scales andleavethe\\ndetails ofhandling machine failures totheprogrammer .\\nBulk Synchronous Programming [17]andsome MPI\\nprimiti ves[11]provide higher -levelabstractions thatmakeiteasier forprogrammers towrite parallel pro-\\ngrams. Akeydifference between these systems and\\nMapReduce isthatMapReduce exploits arestricted pro-\\ngramming model toparallelize theuser program auto-\\nmatically andtoprovide transparent fault-tolerance.\\nOur locality optimization drawsitsinspiration from\\ntechniques such asactivedisks [12,15],where compu-\\ntation ispushed into processing elements thatareclose\\ntolocal disks, toreduce theamount ofdata sent across\\nI/Osubsystems orthenetw ork. Werunoncommodity\\nprocessors towhich asmall number ofdisks aredirectly\\nconnected instead ofrunning directly ondisk controller\\nprocessors, butthegeneral approach issimilar .\\nOur backup task mechanism issimilar totheeager\\nscheduling mechanism emplo yedintheCharlotte Sys-\\ntem [3].One oftheshortcomings ofsimple eager\\nscheduling isthatifagiventaskcauses repeated failures,\\ntheentire computation failstocomplete. We\\x02xsome in-\\nstances ofthisproblem with ourmechanism forskipping\\nbadrecords.\\nTheMapReduce implementation relies onanin-house\\ncluster management system that isresponsible fordis-\\ntributing andrunning user tasks onalargecollection of\\nshared machines. Though notthefocus ofthispaper ,the\\ncluster management system issimilar inspirit toother\\nsystems such asCondor [16].\\nThe sorting facility thatisapart oftheMapReduce\\nlibrary issimilar inoperation toNOW-Sort [1].Source\\nmachines (map workers)partition thedata tobesorted\\nandsend ittooneofRreduce workers. Each reduce\\nworkersorts itsdata locally (inmemory ifpossible). Of\\ncourse NOW-Sort does nothavetheuser-de\\x02nable Map\\nandReduce functions thatmakeourlibrary widely appli-\\ncable.\\nRiver[2]provides aprogramming model where pro-'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 10}, page_content='course NOW-Sort does nothavetheuser-de\\x02nable Map\\nandReduce functions thatmakeourlibrary widely appli-\\ncable.\\nRiver[2]provides aprogramming model where pro-\\ncesses communicate with each other bysending data\\noverdistrib uted queues. LikeMapReduce, theRiver\\nsystem tries toprovide good average case performance\\neveninthepresence ofnon-uniformities introduced by\\nheterogeneous hardw areorsystem perturbations. River\\nachie vesthisbycareful scheduling ofdisk andnetw ork\\ntransfers toachie vebalanced completion times. MapRe-\\nduce hasadifferent approach. Byrestricting thepro-\\ngramming model, theMapReduce frame workisable\\ntopartition theproblem into alargenumber of\\x02ne-\\ngrained tasks. These tasks aredynamically scheduled\\nonavailable workerssothatfaster workersprocess more\\ntasks. The restricted programming model also allows\\nustoschedule redundant executions oftasks near the\\nendofthejobwhich greatly reduces completion time in\\nthepresence ofnon-uniformities (such assloworstuck\\nworkers).\\nBAD-FS [5]hasaverydifferent programming model\\nfrom MapReduce, andunlik eMapReduce, istargeted to\\nToappear inOSDI 2004 11'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 11}, page_content=\"theexecution ofjobs across awide-area netw ork. How-\\never,there aretwofundamental similarities. (1)Both\\nsystems useredundant execution torecoverfrom data\\nloss caused byfailures. (2)Both uselocality-a ware\\nscheduling toreduce theamount ofdata sentacross con-\\ngested netw orklinks.\\nTACC [7]isasystem designed tosimplify con-\\nstruction ofhighly-a vailable netw orkedservices. Like\\nMapReduce, itrelies onre-execution asamechanism for\\nimplementing fault-tolerance.\\n8Conclusions\\nTheMapReduce programming model hasbeen success-\\nfully used atGoogle formanydifferent purposes. We\\nattrib utethissuccess toseveralreasons. First, themodel\\niseasy touse,evenforprogrammers without experience\\nwith parallel anddistrib uted systems, since ithides the\\ndetails ofparallelization, fault-tolerance, locality opti-\\nmization, andload balancing. Second, alargevariety\\nofproblems areeasily expressible asMapReduce com-\\nputations. Forexample, MapReduce isused forthegen-\\neration ofdata forGoogle' sproduction web search ser-\\nvice, forsorting, fordata mining, formachine learning,\\nandmanyother systems. Third, wehavedeveloped an\\nimplementation ofMapReduce thatscales tolargeclus-\\ntersofmachines comprising thousands ofmachines. The\\nimplementation makesef\\x02cient useofthese machine re-\\nsources andtherefore issuitable foruseonmanyofthe\\nlargecomputational problems encountered atGoogle.\\nWehavelearned severalthings from thiswork. First,\\nrestricting theprogramming model makesiteasy topar-\\nallelize anddistrib utecomputations andtomakesuch\\ncomputations fault-tolerant. Second, netw orkbandwidth\\nisascarce resource. Anumber ofoptimizations inour\\nsystem aretherefore targeted atreducing theamount of\\ndatasentacross thenetw ork: thelocality optimization al-\\nlowsustoread data from local disks, andwriting asingle\\ncopyoftheintermediate data tolocal disk savesnetw ork\\nbandwidth. Third, redundant execution canbeused to\\nreduce theimpact ofslowmachines, andtohandle ma-\\nchine failures anddata loss.\\nAckno wledgements\\nJosh Levenber ghasbeen instrumental inrevising and\\nextending theuser-levelMapReduce API with anum-\\nberofnewfeatures based onhisexperience with using\\nMapReduce andother people' ssuggestions forenhance-\\nments. MapReduce reads itsinput from andwrites its\\noutput totheGoogle FileSystem [8].Wewould liketo\\nthank Mohit Aron, HowardGobiof f,Markus Gutschk e,DavidKramer ,Shun-T akLeung, andJosh Redstone for\\ntheir workindeveloping GFS. Wewould also liketo\\nthank PercyLiang andOlcan Sercinoglu fortheir work\\nindeveloping thecluster management system used by\\nMapReduce. MikeBurro ws,Wilson Hsieh, Josh Leven-\\nberg,Sharon Perl, Rob Pike,andDebby Wallach pro-\\nvided helpful comments onearlier drafts ofthis pa-\\nper.Theanon ymous OSDI reviewers, andourshepherd,\\nEric Brewer,provided manyuseful suggestions ofareas\\nwhere thepaper could beimpro ved.Finally ,wethank all\\ntheusers ofMapReduce within Google' sengineering or-\\nganization forproviding helpful feedback, suggestions,\\nandbugreports.\\nRefer ences\\n[1]Andrea C.Arpaci-Dusseau, Remzi H.Arpaci-Dusseau,\\nDavidE.Culler ,Joseph M.Hellerstein, andDavidA.Pat-\\nterson. High-performance sorting onnetw orks ofwork-\\nstations. InProceedings ofthe1997 ACMSIGMOD In-\\nternational Confer ence onMana gement ofData ,Tucson,\\nArizona, May 1997.\\n[2]Remzi H.Arpaci-Dusseau, Eric Anderson, Noah\\nTreuhaft, DavidE.Culler ,Joseph M.Hellerstein, David\\nPatterson, and Kathy Yelick. Cluster I/Owith River:\\nMaking thefastcase common. InProceedings oftheSixth\\nWorkshop onInput/Output inParallel and Distrib uted\\nSystems (IOP ADS '99),pages 10\\x9622, Atlanta, Geor gia,\\nMay 1999.\\n[3]Arash Baratloo, Mehmet Karaul, ZviKedem, andPeter\\nWyckoff.Charlotte: Metacomputing ontheweb.InPro-\\nceedings ofthe9thInternational Confer ence onParallel\\nandDistrib uted Computing Systems ,1996.\\n[4]Luiz A.Barroso, JeffreyDean, andUrsHolzle. Web\\nsearch foraplanet: TheGoogle cluster architecture. IEEE\\nMicr o,23(2):22\\x9628, April 2003.\\n[5]John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau,\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 11}, page_content='[4]Luiz A.Barroso, JeffreyDean, andUrsHolzle. Web\\nsearch foraplanet: TheGoogle cluster architecture. IEEE\\nMicr o,23(2):22\\x9628, April 2003.\\n[5]John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau,\\nRemzi H.Arpaci-Dusseau, andMiron Livny.Explicit\\ncontrol inabatch-a waredistrib uted \\x02lesystem. InPro-\\nceedings ofthe1stUSENIX Symposium onNetwork ed\\nSystems Design andImplementation NSDI ,March 2004.\\n[6]Guy E.Blelloch. Scans asprimiti veparallel operations.\\nIEEE Transactions onComputer s,C-38(11), November\\n1989.\\n[7]Armando Fox, StevenD.Gribble, Yatin Chawathe,\\nEric A.Brewer,andPaulGauthier .Cluster -based scal-\\nable netw orkservices. InProceedings ofthe16th ACM\\nSymposium onOper ating System Principles ,pages 78\\x96\\n91,Saint-Malo, France, 1997.\\n[8]Sanjay Ghema wat,HowardGobiof f,andShun-T akLe-\\nung. TheGoogle \\x02lesystem. In19th Symposium onOp-\\nerating Systems Principles ,pages 29\\x9643, LakeGeor ge,\\nNewYork,2003.\\nToappear inOSDI 2004 12'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 12}, page_content='[9]S.Gorlatch. Systematic ef\\x02cient parallelization ofscan\\nandother listhomomorphisms. InL.Bouge, P.Fraigni-\\naud, A.Mignotte, andY.Robert, editors, Euro-Par\\'96.\\nParallel Processing ,Lecture Notes inComputer Science\\n1124, pages 401\\x96408. Springer -Verlag, 1996.\\n[10] Jim Gray . Sort benchmark home page.\\nhttp://research.microsoft.com/barc/SortBenchmark/.\\n[11] William Gropp, Ewing Lusk, and Anthon ySkjellum.\\nUsing MPI: Portable Parallel Programming with the\\nMessa ge-Passing Interface .MIT Press, Cambridge, MA,\\n1999.\\n[12] L.Huston, R.Sukthankar ,R.Wickremesinghe, M.Satya-\\nnarayanan, G.R.Ganger ,E.Riedel, andA.Ailamaki. Di-\\namond: Astorage architecture forearly discard ininter-\\nactivesearch. InProceedings ofthe2004 USENIX File\\nandStorageTechnolo gies FAST Confer ence,April 2004.\\n[13] Richard E.Ladner andMichael J.Fischer .Parallel pre\\x02x\\ncomputation. Journal oftheACM,27(4):831\\x96838, 1980.\\n[14] Michael O.Rabin. Ef\\x02cient dispersal ofinformation for\\nsecurity ,load balancing andfaulttolerance. Journal of\\ntheACM,36(2):335\\x96348, 1989.\\n[15] Erik Riedel, Christos Faloutsos, Garth A.Gibson, and\\nDavidNagle. Activedisks forlarge-scale data process-\\ning. IEEE Computer ,pages 68\\x9674, June 2001.\\n[16] Douglas Thain, Todd Tannenbaum, and Miron Livny.\\nDistrib uted computing inpractice: The Condor experi-\\nence. Concurr ency andComputation: Practice andEx-\\nperience ,2004.\\n[17] L.G.Valiant. Abridging model forparallel computation.\\nCommunications oftheACM,33(8):103\\x96111, 1997.\\n[18] Jim Wyllie. Spsort: Howtosort aterabyte quickly .\\nhttp://alme1.almaden.ibm.com/cs/spsort.pdf.\\nAWordFrequency\\nThis section contains aprogram thatcounts thenumber\\nofoccurrences ofeach unique wordinasetofinput \\x02les\\nspeci\\x02ed onthecommand line.\\n#include \"mapreduce/mapreduce.h\"\\n//User\\'smapfunction\\nclassWordCounter :publicMapper{\\npublic:\\nvirtual voidMap(const MapInput& input){\\nconststring& text=input.value();\\nconstintn=text.size();\\nfor(inti=0;i<n;){\\n//Skippastleading whitespace\\nwhile((i<n)&&isspace(text[i]))\\ni++;\\n//Findwordend\\nintstart=i;\\nwhile((i<n)&&!isspace(text[i]))\\ni++;if(start<i)\\nEmit(text.substr(start,i-start),\"1\");\\n}\\n}\\n};\\nREGISTER_MAPPER(WordCounter);\\n//User\\'sreducefunction\\nclassAdder:publicReducer {\\nvirtual voidReduce(ReduceInput* input){\\n//Iterate overallentries withthe\\n//samekeyandaddthevalues\\nint64value=0;\\nwhile(!input->done()) {\\nvalue+=StringToInt(input->value());\\ninput->NextValue();\\n}\\n//Emitsumforinput->key()\\nEmit(IntToString(value));\\n}\\n};\\nREGISTER_REDUCER(Adder);\\nintmain(int argc,char**argv){\\nParseCommandLineFlags(argc, argv);\\nMapReduceSpecification spec;\\n//Storelistofinputfilesinto\"spec\"\\nfor(inti=1;i<argc;i++){\\nMapReduceInput* input=spec.add_input();\\ninput->set_format(\"text\");\\ninput->set_filepattern(argv[i]);\\ninput->set_mapper_class(\"WordCounter\");\\n}\\n//Specify theoutputfiles:\\n///gfs/test/freq-00000-of-00100\\n///gfs/test/freq-00001-of-00100\\n//...\\nMapReduceOutput* out=spec.output();\\nout->set_filebase(\"/gfs/test/freq\");\\nout->set_num_tasks(100);\\nout->set_format(\"text\");\\nout->set_reducer_class(\"Adder\");\\n//Optional: dopartial sumswithinmap\\n//taskstosavenetwork bandwidth\\nout->set_combiner_class(\"Adder\");\\n//Tuningparameters: useatmost2000\\n//machines and100MBofmemorypertask\\nspec.set_machines(2000);\\nspec.set_map_megabytes(100);\\nspec.set_reduce_megabytes(100);\\n//Nowrunit\\nMapReduceResult result;\\nif(!MapReduce(spec, &result)) abort();\\n//Done:\\'result\\' structure contains info\\n//aboutcounters, timetaken,numberof\\n//machines used,etc.\\nreturn0;\\n}\\nToappear inOSDI 2004 13')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(PDF_LINK, extract_images=False)\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 0}, page_content=\"MapReduce: Simpli\\x02ed Data Processing onLargeClusters\\nJeffreyDean andSanjay Ghema wat\\njeff@google.com, sanjay@google.com\\nGoogle,Inc.\\nAbstract\\nMapReduce isaprogramming model andanassoci-\\nated implementation forprocessing andgenerating large\\ndata sets. Users specify amap function thatprocesses a\\nkey/valuepairtogenerate asetofintermediate key/value\\npairs, andareduce function thatmergesallintermediate\\nvalues associated with thesame intermediate key.Many\\nrealworld tasks areexpressible inthismodel, asshown\\ninthepaper .\\nPrograms written inthisfunctional style areautomati-\\ncally parallelized andexecuted onalargecluster ofcom-\\nmodity machines. Therun-time system takescare ofthe\\ndetails ofpartitioning theinput data, scheduling thepro-\\ngram' sexecution across asetofmachines, handling ma-\\nchine failures, andmanaging therequired inter-machine\\ncommunication. This allowsprogrammers without any\\nexperience with parallel anddistrib uted systems toeas-\\nilyutilize theresources ofalargedistrib uted system.\\nOur implementation ofMapReduce runs onalarge\\ncluster ofcommodity machines andishighly scalable:\\natypical MapReduce computation processes manyter-\\nabytes ofdata onthousands ofmachines. Programmers\\n\\x02ndthesystem easy touse: hundreds ofMapReduce pro-\\ngrams havebeen implemented andupwards ofonethou-\\nsand MapReduce jobs areexecuted onGoogle' sclusters\\neveryday.\\n1Introduction\\nOverthepast \\x02veyears, theauthors andmanyothers at\\nGoogle haveimplemented hundreds ofspecial-purpose\\ncomputations that process largeamounts ofrawdata,\\nsuch ascrawled documents, web request logs, etc., to\\ncompute various kinds ofderiveddata, such asinverted\\nindices, various representations ofthegraph structure\\nofweb documents, summaries ofthenumber ofpages\\ncrawled perhost, thesetofmost frequent queries inagivenday,etc. Most such computations areconceptu-\\nallystraightforw ard. However,theinput data isusually\\nlargeandthecomputations havetobedistrib uted across\\nhundreds orthousands ofmachines inorder to\\x02nish in\\nareasonable amount oftime. Theissues ofhowtopar-\\nallelize thecomputation, distrib utethedata, andhandle\\nfailures conspire toobscure theoriginal simple compu-\\ntation with largeamounts ofcomple xcode todeal with\\nthese issues.\\nAsareaction tothiscomple xity,wedesigned anew\\nabstraction thatallowsustoexpress thesimple computa-\\ntions wewere trying toperform buthides themessy de-\\ntails ofparallelization, fault-tolerance, data distrib ution\\nandload balancing inalibrary .Our abstraction isin-\\nspired bythemap andreduce primiti vespresent inLisp\\nandmanyother functional languages. Werealized that\\nmost ofourcomputations involvedapplying amap op-\\neration toeach logical \\x93record\\x94 inourinput inorder to\\ncompute asetofintermediate key/value pairs, andthen\\napplying areduce operation toallthevalues thatshared\\nthesame key,inorder tocombine thederiveddata ap-\\npropriately .Our useofafunctional model with user-\\nspeci\\x02ed map andreduce operations allowsustoparal-\\nlelize largecomputations easily andtousere-execution\\nastheprimary mechanism forfaulttolerance.\\nThemajor contrib utions ofthisworkareasimple and\\npowerful interf acethatenables automatic parallelization\\nanddistrib ution oflarge-scale computations, combined\\nwith animplementation ofthisinterf acethat achie ves\\nhigh performance onlargeclusters ofcommodity PCs.\\nSection 2describes thebasic programming model and\\ngivesseveralexamples. Section 3describes animple-\\nmentation oftheMapReduce interf acetailored towards\\nourcluster -based computing environment. Section 4de-\\nscribes several re\\x02nements oftheprogramming model\\nthatwehavefound useful. Section 5hasperformance\\nmeasurements ofourimplementation foravariety of\\ntasks. Section 6explores theuseofMapReduce within\\nGoogle including ourexperiences inusing itasthebasis\\nToappear inOSDI 2004 1\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 1}, page_content='forarewrite ofourproduction indexing system. Sec-\\ntion7discusses related andfuture work.\\n2Programming Model\\nThecomputation takesasetofinput key/value pairs, and\\nproduces asetofoutput key/value pairs. The user of\\ntheMapReduce library expresses thecomputation astwo\\nfunctions: Map andReduce .\\nMap,written bytheuser,takesaninput pairandpro-\\nduces asetofintermediate key/value pairs. TheMapRe-\\nduce library groups together allintermediate values asso-\\nciated with thesame intermediate keyIandpasses them\\ntotheReduce function.\\nTheReduce function, alsowritten bytheuser,accepts\\nanintermediate keyIandasetofvalues forthatkey.It\\nmergestogether these values toform apossibly smaller\\nsetofvalues. Typically justzero oroneoutput value is\\nproduced perReduce invocation. The intermediate val-\\nuesaresupplied totheuser\\' sreduce function viaaniter-\\nator.This allowsustohandle lists ofvalues thataretoo\\nlargeto\\x02tinmemory .\\n2.1 Example\\nConsider theproblem ofcounting thenumber ofoc-\\ncurrences ofeach wordinalargecollection ofdocu-\\nments. Theuser would write code similar tothefollo w-\\ningpseudo-code:\\nmap(String key,Stringvalue):\\n//key:document name\\n//value:document contents\\nforeachwordwinvalue:\\nEmitIntermediate(w, \"1\");\\nreduce(String key,Iterator values):\\n//key:aword\\n//values: alistofcounts\\nintresult=0;\\nforeachvinvalues:\\nresult+=ParseInt(v);\\nEmit(AsString(result));\\nThemap function emits each wordplus anassociated\\ncount ofoccurrences (just `1\\'inthissimple example).\\nThereduce function sums together allcounts emitted\\nforaparticular word.\\nInaddition, theuser writes code to\\x02llinamapr educe\\nspeci\\x02cation object with thenames oftheinput andout-\\nput\\x02les, andoptional tuning parameters. Theuser then\\ninvokestheMapReduce function, passing itthespeci\\x02-\\ncation object. Theuser\\' scode islinkedtogether with the\\nMapReduce library (implemented inC++). Appendix A\\ncontains thefullprogram textforthisexample.2.2 Types\\nEventhough theprevious pseudo-code iswritten interms\\nofstring inputs andoutputs, conceptually themap and\\nreduce functions supplied bytheuser haveassociated\\ntypes:\\nmap(k1,v1) !list(k2,v2)\\nreduce(k2,list(v2)) !list(v2)\\nI.e.,theinput keysandvalues aredrawnfrom adifferent\\ndomain than theoutput keysandvalues. Furthermore,\\ntheintermediate keysandvalues arefrom thesame do-\\nmain astheoutput keysandvalues.\\nOur C++ implementation passes strings toandfrom\\ntheuser-de\\x02ned functions andleavesittotheuser code\\ntoconvertbetween strings andappropriate types.\\n2.3 MoreExamples\\nHere areafewsimple examples ofinteresting programs\\nthat canbeeasily expressed asMapReduce computa-\\ntions.\\nDistrib uted Grep: Themap function emits alineifit\\nmatches asupplied pattern. The reduce function isan\\nidentity function thatjustcopies thesupplied intermedi-\\natedata totheoutput.\\nCount ofURL Access Frequency: The map func-\\ntion processes logs ofweb page requests and outputs\\nhURL;1i.The reduce function adds together allvalues\\nforthesame URL andemits ahURL;totalcount i\\npair.\\nReverse Web-Link Graph: Themap function outputs\\nhtarget ;source ipairs foreach link toatarget\\nURL found inapage namedsource .The reduce\\nfunction concatenates thelistofallsource URLs as-\\nsociated with agiventargetURL and emits thepair:\\nhtarget ;list(source )i\\nTerm-V ector perHost: Aterm vector summarizes the\\nmost important words thatoccur inadocument oraset\\nofdocuments asalistofhword;frequency ipairs. The\\nmap function emits ahhostname ;termvector i\\npair foreach input document (where thehostname is\\nextracted from theURL ofthedocument). The re-\\nduce function ispassed allper-document term vectors\\nforagivenhost. Itadds these term vectors together ,\\nthrowing awayinfrequent terms, andthen emits a\\x02nal\\nhhostname ;termvector ipair.\\nToappear inOSDI 2004 2'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 2}, page_content='User\\nProgram\\nMaster(1) fork\\nworker(1) fork\\nworker(1) fork\\n(2)\\nassign\\nmap(2)\\nassign\\nreduce\\nsplit 0\\nsplit 1\\nsplit 2\\nsplit 3\\nsplit 4  \\noutput\\nfile 0    (6) write\\nworker(3) read\\nworker  (4) local write\\n  \\nMap\\nphaseIntermediate files\\n(on local disks)workeroutput\\nfile 1\\nInput\\nfiles(5) remote read\\nReduce\\nphaseOutput\\nfiles\\nFigure 1:Execution overvie w\\nInverted Index: The map function parses each docu-\\nment, andemits asequence ofhword;document IDi\\npairs. The reduce function accepts allpairs foragiven\\nword,sorts thecorresponding document IDsandemits a\\nhword;list(document ID)ipair.Thesetofalloutput\\npairs forms asimple inverted index.Itiseasy toaugment\\nthiscomputation tokeeptrack ofwordpositions.\\nDistrib uted Sort: The map function extracts thekey\\nfrom each record, andemits ahkey;record ipair.The\\nreduce function emits allpairs unchanged. This compu-\\ntation depends onthepartitioning facilities described in\\nSection 4.1andtheordering properties described inSec-\\ntion4.2.\\n3Implementation\\nManydifferent implementations oftheMapReduce in-\\nterfacearepossible. The right choice depends onthe\\nenvironment. Forexample, oneimplementation may be\\nsuitable forasmall shared-memory machine, another for\\nalargeNUMA multi-processor ,andyetanother foran\\nevenlargercollection ofnetw orkedmachines.\\nThis section describes animplementation targeted\\ntothecomputing environment inwide useatGoogle:largeclusters ofcommodity PCsconnected together with\\nswitched Ethernet [4].Inourenvironment:\\n(1)Machines aretypically dual-processor x86processors\\nrunning Linux, with 2-4GBofmemory permachine.\\n(2)Commodity netw orking hardw areisused \\x96typically\\neither 100 megabits/second or1gigabit/second atthe\\nmachine level,butaveraging considerably lessinover-\\nallbisection bandwidth.\\n(3)Acluster consists ofhundreds orthousands ofma-\\nchines, andtherefore machine failures arecommon.\\n(4)Storage isprovided byinexpensi veIDE disks at-\\ntached directly toindividual machines. Adistrib uted \\x02le\\nsystem [8]developed in-house isused tomanage thedata\\nstored onthese disks. The\\x02lesystem uses replication to\\nprovide availability andreliability ontopofunreliable\\nhardw are.\\n(5)Users submit jobs toascheduling system. Each job\\nconsists ofasetoftasks, andismapped bythescheduler\\ntoasetofavailable machines within acluster .\\n3.1 Execution Overview\\nThe Map invocations aredistrib uted across multiple\\nmachines byautomatically partitioning theinput data\\nToappear inOSDI 2004 3'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 3}, page_content=\"into asetofMsplits .The input splits canbepro-\\ncessed inparallel bydifferent machines. Reduce invoca-\\ntions aredistrib uted bypartitioning theintermediate key\\nspace intoRpieces using apartitioning function (e.g.,\\nhash(key)modR).Thenumber ofpartitions (R)and\\nthepartitioning function arespeci\\x02ed bytheuser.\\nFigure 1showstheoverall \\x03owofaMapReduce op-\\neration inourimplementation. When theuser program\\ncalls theMapReduce function, thefollo wing sequence\\nofactions occurs (thenumbered labels inFigure 1corre-\\nspond tothenumbers inthelistbelow):\\n1.The MapReduce library intheuser program \\x02rst\\nsplits theinput \\x02les intoMpieces oftypically 16\\nmegabytes to64megabytes (MB) perpiece (con-\\ntrollable bytheuser viaanoptional parameter). It\\nthen starts upmanycopies oftheprogram onaclus-\\nterofmachines.\\n2.One ofthecopies oftheprogram isspecial \\x96the\\nmaster .Therestareworkersthatareassigned work\\nbythemaster .There areMmap tasks andRreduce\\ntasks toassign. Themaster picks idleworkersand\\nassigns each oneamap taskorareduce task.\\n3.Aworkerwho isassigned amap task reads the\\ncontents ofthecorresponding input split. Itparses\\nkey/valuepairs outoftheinput data andpasses each\\npairtotheuser-de\\x02ned Map function. Theinterme-\\ndiate key/value pairs produced bytheMap function\\narebuffered inmemory .\\n4.Periodically ,thebuffered pairs arewritten tolocal\\ndisk, partitioned intoRregions bythepartitioning\\nfunction. The locations ofthese buffered pairs on\\nthelocal disk arepassed back tothemaster ,who\\nisresponsible forforw arding these locations tothe\\nreduce workers.\\n5.When areduce workerisnoti\\x02ed bythemaster\\nabout these locations, ituses remote procedure calls\\ntoread thebuffered data from thelocal disks ofthe\\nmap workers.When areduce workerhasread allin-\\ntermediate data, itsorts itbytheintermediate keys\\nsothatalloccurrences ofthesame keyaregrouped\\ntogether .The sorting isneeded because typically\\nmanydifferent keysmap tothesame reduce task. If\\ntheamount ofintermediate data istoolargeto\\x02tin\\nmemory ,anexternal sortisused.\\n6.Thereduce workeriterates overthesorted interme-\\ndiate data andforeach unique intermediate keyen-\\ncountered, itpasses thekeyandthecorresponding\\nsetofintermediate values totheuser' sReduce func-\\ntion. Theoutput oftheReduce function isappended\\ntoa\\x02nal output \\x02leforthisreduce partition.7.When allmap tasks and reduce tasks havebeen\\ncompleted, themaster wakesuptheuser program.\\nAtthispoint, theMapReduce callintheuser pro-\\ngram returns back totheuser code.\\nAfter successful completion, theoutput ofthemapre-\\nduce execution isavailable intheRoutput \\x02les (one per\\nreduce task, with \\x02lenames asspeci\\x02ed bytheuser).\\nTypically ,users donotneed tocombine theseRoutput\\n\\x02les intoone\\x02le\\x96theyoften pass these \\x02les asinput to\\nanother MapReduce call, orusethem from another dis-\\ntributed application thatisable todeal with input thatis\\npartitioned intomultiple \\x02les.\\n3.2 Master Data Structur es\\nThemaster keeps severaldata structures. Foreach map\\ntaskandreduce task, itstores thestate (idle,in-pr ogress,\\norcompleted ),andtheidentity oftheworkermachine\\n(fornon-idle tasks).\\nThemaster istheconduit through which thelocation\\nofintermediate \\x02leregions ispropagated from map tasks\\ntoreduce tasks. Therefore, foreach completed map task,\\nthemaster stores thelocations andsizes oftheRinter-\\nmediate \\x02leregions produced bythemap task. Updates\\ntothislocation andsizeinformation arerecei vedasmap\\ntasks arecompleted. The information ispushed incre-\\nmentally toworkersthathavein-pr ogressreduce tasks.\\n3.3 Fault Tolerance\\nSince theMapReduce library isdesigned tohelp process\\nverylargeamounts ofdata using hundreds orthousands\\nofmachines, thelibrary must tolerate machine failures\\ngracefully .\\nWorkerFailur e\\nThe master pings everyworkerperiodically .Ifnore-\\nsponse isrecei vedfrom aworkerinacertain amount of\\ntime, themaster marks theworkerasfailed. Anymap\\ntasks completed bytheworkerarereset back totheir ini-\\ntialidlestate, andtherefore become eligible forschedul-\\ningonother workers.Similarly ,anymap task orreduce\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 3}, page_content='tasks completed bytheworkerarereset back totheir ini-\\ntialidlestate, andtherefore become eligible forschedul-\\ningonother workers.Similarly ,anymap task orreduce\\ntask inprogress onafailed workerisalso reset toidle\\nandbecomes eligible forrescheduling.\\nCompleted map tasks arere-executed onafailure be-\\ncause their output isstored onthelocal disk(s) ofthe\\nfailed machine andistherefore inaccessible. Completed\\nreduce tasks donotneed tobere-executed since their\\noutput isstored inaglobal \\x02lesystem.\\nWhen amap task isexecuted \\x02rst byworkerAand\\nthen later executed byworkerB(because Afailed), all\\nToappear inOSDI 2004 4'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 4}, page_content=\"workersexecuting reduce tasks arenoti\\x02ed ofthere-\\nexecution. Anyreduce task thathasnotalready read the\\ndata from workerAwillread thedata from workerB.\\nMapReduce isresilient tolarge-scale workerfailures.\\nForexample, during oneMapReduce operation, netw ork\\nmaintenance onarunning cluster wascausing groups of\\n80machines atatime tobecome unreachable forsev-\\neralminutes. TheMapReduce master simply re-executed\\ntheworkdone bytheunreachable workermachines, and\\ncontinued tomakeforw ardprogress, eventually complet-\\ningtheMapReduce operation.\\nMaster Failur e\\nItiseasy tomakethemaster write periodic checkpoints\\nofthemaster data structures described above.Ifthemas-\\ntertask dies, anewcopycanbestarted from thelast\\ncheckpointed state. However,giventhatthere isonly a\\nsingle master ,itsfailure isunlik ely; therefore ourcur-\\nrentimplementation aborts theMapReduce computation\\nifthemaster fails. Clients cancheck forthiscondition\\nandretry theMapReduce operation iftheydesire.\\nSemantics inthePresence ofFailur es\\nWhen theuser-supplied map andreduce operators arede-\\nterministic functions oftheir input values, ourdistrib uted\\nimplementation produces thesame output aswould have\\nbeen produced byanon-f aulting sequential execution of\\ntheentire program.\\nWerely onatomic commits ofmap andreduce task\\noutputs toachie vethisproperty .Each in-progress task\\nwrites itsoutput toprivatetemporary \\x02les. Areduce task\\nproduces onesuch \\x02le,andamap task produces Rsuch\\n\\x02les (one perreduce task). When amap task completes,\\ntheworkersends amessage tothemaster andincludes\\nthenames oftheRtemporary \\x02les inthemessage. If\\nthemaster recei vesacompletion message foranalready\\ncompleted map task, itignores themessage. Otherwise,\\nitrecords thenames ofR\\x02les inamaster data structure.\\nWhen areduce task completes, thereduce worker\\natomically renames itstemporary output \\x02letothe\\x02nal\\noutput \\x02le.Ifthesame reduce taskisexecuted onmulti-\\nplemachines, multiple rename calls willbeexecuted for\\nthesame \\x02nal output \\x02le.Werelyontheatomic rename\\noperation provided bytheunderlying \\x02lesystem toguar-\\nantee thatthe\\x02nal \\x02lesystem state contains justthedata\\nproduced byoneexecution ofthereduce task.\\nThevastmajority ofourmap andreduce operators are\\ndeterministic, andthefactthatoursemantics areequiv-\\nalent toasequential execution inthiscase makesitveryeasy forprogrammers toreason about their program' sbe-\\nhavior.When themap and/or reduce operators arenon-\\ndeterministic, weprovide weak erbutstillreasonable se-\\nmantics. Inthepresence ofnon-deterministic operators,\\ntheoutput ofaparticular reduce taskR1isequivalent to\\ntheoutput forR1produced byasequential execution of\\nthenon-deterministic program. However,theoutput for\\nadifferent reduce taskR2may correspond totheoutput\\nforR2produced byadifferent sequential execution of\\nthenon-deterministic program.\\nConsider map taskMandreduce tasksR1andR2.\\nLete(Ri)betheexecution ofRithatcommitted (there\\nisexactly onesuch execution). The weak ersemantics\\narise because e(R1)may haveread theoutput produced\\nbyoneexecution ofMande(R2)may haveread the\\noutput produced byadifferent execution ofM.\\n3.4 Locality\\nNetw orkbandwidth isarelati velyscarce resource inour\\ncomputing environment. Weconserv enetw orkband-\\nwidth bytaking advantage ofthefactthattheinput data\\n(managed byGFS [8])isstored onthelocal disks ofthe\\nmachines that makeupourcluster .GFS divides each\\n\\x02leinto64MBblocks, andstores severalcopies ofeach\\nblock (typically 3copies) ondifferent machines. The\\nMapReduce master takesthelocation information ofthe\\ninput \\x02les into account andattempts toschedule amap\\ntask onamachine thatcontains areplica ofthecorre-\\nsponding input data. Failing that, itattempts toschedule\\namap tasknear areplica ofthattask' sinput data (e.g., on\\naworkermachine thatisonthesame netw orkswitch as\\nthemachine containing thedata). When running large\\nMapReduce operations onasigni\\x02cant fraction ofthe\\nworkersinacluster ,most input data isread locally and\\nconsumes nonetw orkbandwidth.\\n3.5 TaskGranularity\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 4}, page_content='MapReduce operations onasigni\\x02cant fraction ofthe\\nworkersinacluster ,most input data isread locally and\\nconsumes nonetw orkbandwidth.\\n3.5 TaskGranularity\\nWesubdi vide themap phase intoMpieces andthere-\\nduce phase intoRpieces, asdescribed above.Ideally ,M\\nandRshould bemuch largerthan thenumber ofworker\\nmachines. Having each workerperform manydifferent\\ntasks impro vesdynamic load balancing, andalso speeds\\nuprecoverywhen aworkerfails: themanymap tasks\\nithascompleted canbespread outacross alltheother\\nworkermachines.\\nThere arepractical bounds onhowlargeMandRcan\\nbeinourimplementation, since themaster must make\\nO(M+R)scheduling decisions andkeepsO(M\\x03R)\\nstate inmemory asdescribed above.(The constant fac-\\ntorsformemory usage aresmall however:theO(M\\x03R)\\npiece ofthestate consists ofapproximately onebyte of\\ndata permap task/reduce taskpair.)\\nToappear inOSDI 2004 5'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 5}, page_content='Furthermore, Risoften constrained byusers because\\ntheoutput ofeach reduce task ends upinaseparate out-\\nput\\x02le. Inpractice, wetend tochoose Msothateach\\nindividual taskisroughly 16MBto64MBofinput data\\n(sothatthelocality optimization described aboveismost\\neffective),andwemakeRasmall multiple ofthenum-\\nberofworkermachines weexpect touse. Weoften per-\\nform MapReduce computations withM=200;000and\\nR=5;000,using 2,000 workermachines.\\n3.6 Backup Tasks\\nOne ofthecommon causes thatlengthens thetotal time\\ntakenforaMapReduce operation isa\\x93straggler\\x94: ama-\\nchine thattakesanunusually long time tocomplete one\\nofthelastfewmap orreduce tasks inthecomputation.\\nStragglers canarise forawhole host ofreasons. Forex-\\nample, amachine with abaddisk may experience fre-\\nquent correctable errors thatslowitsread performance\\nfrom 30MB/s to1MB/s. The cluster scheduling sys-\\ntem may havescheduled other tasks onthemachine,\\ncausing ittoexecute theMapReduce code more slowly\\nduetocompetition forCPU, memory ,local disk, ornet-\\nworkbandwidth. Arecent problem weexperienced was\\nabuginmachine initialization code thatcaused proces-\\nsorcaches tobedisabled: computations onaffected ma-\\nchines slowed downbyoverafactor ofonehundred.\\nWehaveageneral mechanism toalleviate theprob-\\nlemofstragglers. When aMapReduce operation isclose\\ntocompletion, themaster schedules backup executions\\noftheremaining in-pr ogresstasks. The task ismark ed\\nascompleted whene vereither theprimary orthebackup\\nexecution completes. Wehavetuned thismechanism so\\nthat ittypically increases thecomputational resources\\nused bytheoperation bynomore than afewpercent.\\nWehavefound that thissigni\\x02cantly reduces thetime\\ntocomplete largeMapReduce operations. Asanexam-\\nple,thesortprogram described inSection 5.3takes44%\\nlonger tocomplete when thebackup task mechanism is\\ndisabled.\\n4Re\\x02nements\\nAlthough thebasic functionality provided bysimply\\nwriting Map andReduce functions issuf\\x02cient formost\\nneeds, wehavefound afewextensions useful. These are\\ndescribed inthissection.\\n4.1 Partitioning Function\\nThe users ofMapReduce specify thenumber ofreduce\\ntasks/output \\x02les thattheydesire (R).Data gets parti-\\ntioned across these tasks using apartitioning function ontheintermediate key.Adefaultpartitioning function is\\nprovided thatuses hashing (e.g. \\x93hash(key)modR\\x94).\\nThis tends toresult infairly well-balanced partitions. In\\nsome cases, however,itisuseful topartition data by\\nsome other function ofthekey.Forexample, sometimes\\ntheoutput keysareURLs, andwewantallentries fora\\nsingle host toendupinthesame output \\x02le. Tosupport\\nsituations likethis, theuser oftheMapReduce library\\ncanprovide aspecial partitioning function. Forexample,\\nusing \\x93hash(Hostname (urlkey))modR\\x94asthepar-\\ntitioning function causes allURLs from thesame host to\\nendupinthesame output \\x02le.\\n4.2 Ordering Guarantees\\nWeguarantee thatwithin agivenpartition, theinterme-\\ndiate key/value pairs areprocessed inincreasing keyor-\\nder.This ordering guarantee makesiteasy togenerate\\nasorted output \\x02leperpartition, which isuseful when\\ntheoutput \\x02leformat needs tosupport ef\\x02cient random\\naccess lookups bykey,orusers oftheoutput \\x02nditcon-\\nvenient tohavethedata sorted.\\n4.3 Combiner Function\\nInsome cases, there issigni\\x02cant repetition intheinter-\\nmediate keysproduced byeach map task, andtheuser-\\nspeci\\x02ed Reduce function iscommutati veandassocia-\\ntive.Agood example ofthisisthewordcounting exam-\\npleinSection 2.1.Since wordfrequencies tend tofollo w\\naZipf distrib ution, each map taskwillproduce hundreds\\northousands ofrecords oftheform<the,1>.Allof\\nthese counts willbesentoverthenetw orktoasingle re-\\nduce taskandthen added together bytheReduce function\\ntoproduce onenumber .Weallowtheuser tospecify an\\noptional Combiner function thatdoes partial merging of\\nthisdata before itissentoverthenetw ork.\\nThe Combiner function isexecuted oneach machine\\nthatperforms amap task. Typically thesame code isused\\ntoimplement both thecombiner andthereduce func-\\ntions. Theonly difference between areduce function and'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 5}, page_content='thatperforms amap task. Typically thesame code isused\\ntoimplement both thecombiner andthereduce func-\\ntions. Theonly difference between areduce function and\\nacombiner function ishowtheMapReduce library han-\\ndles theoutput ofthefunction. The output ofareduce\\nfunction iswritten tothe\\x02nal output \\x02le. Theoutput of\\nacombiner function iswritten toanintermediate \\x02lethat\\nwillbesenttoareduce task.\\nPartial combining signi\\x02cantly speeds upcertain\\nclasses ofMapReduce operations. Appendix Acontains\\nanexample thatuses acombiner .\\n4.4 Input andOutput Types\\nTheMapReduce library provides support forreading in-\\nputdata inseveraldifferent formats. Forexample, \\x93text\\x94\\nToappear inOSDI 2004 6'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 6}, page_content=\"mode input treats each lineasakey/value pair: thekey\\nistheoffsetinthe\\x02leandthevalue isthecontents of\\ntheline. Another common supported format stores a\\nsequence ofkey/value pairs sorted bykey.Each input\\ntype implementation knowshowtosplit itself intomean-\\ningful ranges forprocessing asseparate map tasks (e.g.\\ntextmode' srange splitting ensures thatrange splits oc-\\ncuronly atlineboundaries). Users canaddsupport fora\\nnewinput type byproviding animplementation ofasim-\\nplereader interf ace,though most users justuseoneofa\\nsmall number ofprede\\x02ned input types.\\nAreader does notnecessarily need toprovide data\\nread from a\\x02le.Forexample, itiseasy tode\\x02ne areader\\nthatreads records from adatabase, orfrom data struc-\\ntures mapped inmemory .\\nInasimilar fashion, wesupport asetofoutput types\\nforproducing data indifferent formats anditiseasy for\\nuser code toaddsupport fornewoutput types.\\n4.5 Side-effects\\nInsome cases, users ofMapReduce havefound itcon-\\nvenient toproduce auxiliary \\x02les asadditional outputs\\nfrom their map and/or reduce operators. Werelyonthe\\napplication writer tomakesuch side-ef fects atomic and\\nidempotent. Typically theapplication writes toatempo-\\nrary\\x02leandatomically renames this\\x02leonce ithasbeen\\nfully generated.\\nWedonotprovide support foratomic two-phase com-\\nmits ofmultiple output \\x02les produced byasingle task.\\nTherefore, tasks thatproduce multiple output \\x02les with\\ncross-\\x02le consistenc yrequirements should bedetermin-\\nistic. This restriction hasneverbeen anissue inpractice.\\n4.6 Skipping Bad Records\\nSometimes there arebugsinusercode thatcause theMap\\norReduce functions tocrash deterministically oncertain\\nrecords. Such bugspreventaMapReduce operation from\\ncompleting. Theusual course ofaction isto\\x02xthebug,\\nbutsometimes thisisnotfeasible; perhaps thebugisin\\nathird-party library forwhich source code isunavail-\\nable. Also, sometimes itisacceptable toignore afew\\nrecords, forexample when doing statistical analysis on\\nalargedata set.Weprovide anoptional mode ofexecu-\\ntionwhere theMapReduce library detects which records\\ncause deterministic crashes andskips these records inor-\\ndertomakeforw ardprogress.\\nEach workerprocess installs asignal handler that\\ncatches segmentation violations andbuserrors. Before\\ninvoking auser Map orReduce operation, theMapRe-\\nduce library stores thesequence number oftheargument\\ninaglobal variable. Iftheuser code generates asignal,thesignal handler sends a\\x93last gasp\\x94 UDP pack etthat\\ncontains thesequence number totheMapReduce mas-\\nter.When themaster hasseen more than onefailure on\\naparticular record, itindicates thattherecord should be\\nskipped when itissues thenextre-execution ofthecorre-\\nsponding Map orReduce task.\\n4.7 Local Execution\\nDebugging problems inMap orReduce functions canbe\\ntricky,since theactual computation happens inadis-\\ntributed system, often onseveral thousand machines,\\nwith workassignment decisions made dynamically by\\nthemaster .Tohelp facilitate debugging, pro\\x02ling, and\\nsmall-scale testing, wehavedeveloped analternati veim-\\nplementation oftheMapReduce library thatsequentially\\nexecutes alloftheworkforaMapReduce operation on\\nthelocal machine. Controls areprovided totheuser so\\nthat thecomputation canbelimited toparticular map\\ntasks. Users invoketheir program with aspecial \\x03agand\\ncanthen easily useanydebugging ortesting tools they\\n\\x02nduseful (e.g.gdb).\\n4.8 Status Information\\nThe master runs aninternal HTTP serverandexports\\nasetofstatus pages forhuman consumption. The sta-\\ntuspages showtheprogress ofthecomputation, such as\\nhowmanytasks havebeen completed, howmanyarein\\nprogress, bytes ofinput, bytes ofintermediate data, bytes\\nofoutput, processing rates, etc. Thepages also contain\\nlinks tothestandard error andstandard output \\x02les gen-\\nerated byeach task. The user canusethisdata topre-\\ndicthowlong thecomputation willtake,andwhether or\\nnotmore resources should beadded tothecomputation.\\nThese pages canalsobeused to\\x02gure outwhen thecom-\\nputation ismuch slowerthan expected.\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 6}, page_content='dicthowlong thecomputation willtake,andwhether or\\nnotmore resources should beadded tothecomputation.\\nThese pages canalsobeused to\\x02gure outwhen thecom-\\nputation ismuch slowerthan expected.\\nInaddition, thetop-le velstatus page showswhich\\nworkershavefailed, andwhich map andreduce tasks\\ntheywere processing when theyfailed. This informa-\\ntion isuseful when attempting todiagnose bugsinthe\\nuser code.\\n4.9 Counters\\nThe MapReduce library provides acounter facility to\\ncount occurrences ofvarious events. Forexample, user\\ncode may wanttocount total number ofwords processed\\northenumber ofGerman documents indexed,etc.\\nTousethisfacility ,user code creates anamed counter\\nobject andthen increments thecounter appropriately in\\ntheMap and/or Reduce function. Forexample:\\nToappear inOSDI 2004 7'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 7}, page_content='Counter* uppercase;\\nuppercase =GetCounter(\"uppercase\");\\nmap(String name,Stringcontents):\\nforeachwordwincontents:\\nif(IsCapitalized(w)):\\nuppercase->Increment();\\nEmitIntermediate(w, \"1\");\\nThe counter values from individual workermachines\\nareperiodically propagated tothemaster (piggyback ed\\nontheping response). Themaster aggre gates thecounter\\nvalues from successful map andreduce tasks andreturns\\nthem totheuser code when theMapReduce operation\\niscompleted. The current counter values arealso dis-\\nplayed onthemaster status page sothat ahuman can\\nwatch theprogress ofthelivecomputation. When aggre-\\ngating counter values, themaster eliminates theeffects of\\nduplicate executions ofthesame map orreduce task to\\navoiddouble counting. (Duplicate executions canarise\\nfrom ouruseofbackup tasks andfrom re-execution of\\ntasks duetofailures.)\\nSome counter values areautomatically maintained\\nbytheMapReduce library ,such asthenumber ofin-\\nputkey/value pairs processed andthenumber ofoutput\\nkey/value pairs produced.\\nUsers havefound thecounter facility useful forsan-\\nitychecking thebeha viorofMapReduce operations. For\\nexample, insome MapReduce operations, theuser code\\nmay wanttoensure that thenumber ofoutput pairs\\nproduced exactly equals thenumber ofinput pairs pro-\\ncessed, orthatthefraction ofGerman documents pro-\\ncessed iswithin some tolerable fraction ofthetotal num-\\nberofdocuments processed.\\n5Performance\\nInthissection wemeasure theperformance ofMapRe-\\nduce ontwocomputations running onalargecluster of\\nmachines. One computation searches through approxi-\\nmately oneterabyte ofdata looking foraparticular pat-\\ntern. Theother computation sorts approximately oneter-\\nabyte ofdata.\\nThese twoprograms arerepresentati veofalargesub-\\nsetoftherealprograms written byusers ofMapReduce \\x96\\noneclass ofprograms shuf\\x03esdata from onerepresenta-\\ntiontoanother ,andanother class extracts asmall amount\\nofinteresting data from alargedata set.\\n5.1 Cluster Con\\x02guration\\nAlloftheprograms were executed onacluster that\\nconsisted ofapproximately 1800 machines. Each ma-\\nchine hadtwo2GHz Intel Xeon processors with Hyper -\\nThreading enabled, 4GB ofmemory ,two160GB IDE20 40 60 80 100\\nSeconds0100002000030000Input (MB/s)\\nFigure 2:Data transfer rateovertime\\ndisks, andagigabit Ethernet link. The machines were\\narranged inatwo-leveltree-shaped switched netw ork\\nwith approximately 100-200 Gbps ofaggre gate band-\\nwidth available attheroot. Allofthemachines were\\ninthesame hosting facility andtherefore theround-trip\\ntime between anypairofmachines waslessthan amil-\\nlisecond.\\nOutofthe4GB ofmemory ,approximately 1-1.5GB\\nwasreserv edbyother tasks running onthecluster .The\\nprograms were executed onaweek endafternoon, when\\ntheCPUs, disks, andnetw orkwere mostly idle.\\n5.2 Grep\\nThegrepprogram scans through 1010100-byte records,\\nsearching forarelati velyrarethree-character pattern (the\\npattern occurs in92,337 records). Theinput issplit into\\napproximately 64MB pieces (M=15000 ),andtheen-\\ntireoutput isplaced inone\\x02le(R=1).\\nFigure 2showstheprogress ofthecomputation over\\ntime. TheY-axis showstherateatwhich theinput data is\\nscanned. Therategradually picks upasmore machines\\nareassigned tothisMapReduce computation, andpeaks\\natover30GB/s when 1764 workershavebeen assigned.\\nAsthemap tasks \\x02nish, theratestarts dropping andhits\\nzero about 80seconds intothecomputation. Theentire\\ncomputation takesapproximately 150seconds from start\\nto\\x02nish. This includes about aminute ofstartup over-\\nhead. Theoverhead isduetothepropagation ofthepro-\\ngram toallworkermachines, anddelays interacting with\\nGFS toopen thesetof1000 input \\x02les andtogetthe\\ninformation needed forthelocality optimization.\\n5.3 Sort\\nThesort program sorts1010100-byte records (approxi-\\nmately 1terabyte ofdata). This program ismodeled after\\ntheTeraSort benchmark [10].\\nThe sorting program consists oflessthan 50lines of\\nuser code. Athree-line Map function extracts a10-byte\\nsorting keyfrom atextlineandemits thekeyandthe\\nToappear inOSDI 2004 8'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 8}, page_content='500 100005000100001500020000Input (MB/s)\\n500 100005000100001500020000Shuffle (MB/s)\\n500 1000\\nSeconds05000100001500020000Output (MB/s)Done\\n(a)Normal execution500 100005000100001500020000Input (MB/s)\\n500 100005000100001500020000Shuffle (MB/s)\\n500 1000\\nSeconds05000100001500020000Output (MB/s)Done\\n(b)Nobackup tasks500 100005000100001500020000Input (MB/s)\\n500 100005000100001500020000Shuffle (MB/s)\\n500 1000\\nSeconds05000100001500020000Output (MB/s)Done\\n(c)200tasks killed\\nFigure 3:Data transfer rates overtime fordifferent executions ofthesortprogram\\noriginal textlineastheintermediate key/value pair.We\\nused abuilt-in Identity function astheReduce operator .\\nThis functions passes theintermediate key/valuepairun-\\nchanged astheoutput key/value pair.The \\x02nal sorted\\noutput iswritten toasetof2-wayreplicated GFS \\x02les\\n(i.e., 2terabytes arewritten astheoutput oftheprogram).\\nAsbefore, theinput data issplit into 64MB pieces\\n(M=15000 ).Wepartition thesorted output into4000\\n\\x02les (R=4000 ).Thepartitioning function uses theini-\\ntialbytes ofthekeytosegregate itintooneofRpieces.\\nOurpartitioning function forthisbenchmark hasbuilt-\\ninknowledge ofthedistrib ution ofkeys.Inageneral\\nsorting program, wewould addapre-pass MapReduce\\noperation that would collect asample ofthekeysand\\nusethedistrib ution ofthesampled keystocompute split-\\npoints forthe\\x02nal sorting pass.\\nFigure 3(a)showstheprogress ofanormal execution\\nofthesortprogram. The top-left graph showstherate\\natwhich input isread. Theratepeaks atabout 13GB/s\\nanddies offfairly quickly since allmap tasks \\x02nish be-\\nfore 200seconds haveelapsed. Note thattheinput rate\\nislessthan forgrep.This isbecause thesortmap tasks\\nspend about halftheir time andI/Obandwidth writing in-\\ntermediate output totheir local disks. Thecorresponding\\nintermediate output forgrep hadnegligible size.\\nThe middle-left graph showstherate atwhich data\\nissent overthenetw orkfrom themap tasks tothere-\\nduce tasks. This shuf\\x03ing starts assoon asthe\\x02rst\\nmap task completes. The \\x02rst hump inthegraph isforthe\\x02rst batch ofapproximately 1700 reduce tasks (the\\nentire MapReduce wasassigned about 1700 machines,\\nandeach machine executes atmost onereduce task ata\\ntime). Roughly 300seconds intothecomputation, some\\nofthese \\x02rst batch ofreduce tasks \\x02nish andwestart\\nshuf\\x03ing data fortheremaining reduce tasks. Allofthe\\nshuf\\x03ing isdone about 600seconds intothecomputation.\\nThebottom-left graph showstherateatwhich sorted\\ndata iswritten tothe\\x02nal output \\x02les bythereduce tasks.\\nThere isadelay between theendofthe\\x02rstshuf\\x03ing pe-\\nriod andthestart ofthewriting period because thema-\\nchines arebusysorting theintermediate data. Thewrites\\ncontinue atarateofabout 2-4GB/s forawhile. Allof\\nthewrites \\x02nish about 850seconds intothecomputation.\\nIncluding startup overhead, theentire computation takes\\n891seconds. This issimilar tothecurrent best reported\\nresult of1057 seconds fortheTeraSort benchmark [18].\\nAfewthings tonote: theinput rateishigher than the\\nshuf\\x03erateandtheoutput ratebecause ofourlocality\\noptimization \\x96most data isread from alocal disk and\\nbypasses ourrelati velybandwidth constrained netw ork.\\nThe shuf\\x03erate ishigher than theoutput rate because\\ntheoutput phase writes twocopies ofthesorted data (we\\nmaketworeplicas oftheoutput forreliability andavail-\\nability reasons). Wewrite tworeplicas because thatis\\nthemechanism forreliability andavailability provided\\nbyourunderlying \\x02lesystem. Netw orkbandwidth re-\\nquirements forwriting data would bereduced iftheun-\\nderlying \\x02lesystem used erasure coding [14]rather than\\nreplication.\\nToappear inOSDI 2004 9'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 9}, page_content=\"5.4 Effect ofBackup Tasks\\nInFigure 3(b),weshowanexecution ofthesortpro-\\ngram with backup tasks disabled. Theexecution \\x03owis\\nsimilar tothatshowninFigure 3(a),except thatthere is\\naverylong tailwhere hardly anywrite activity occurs.\\nAfter 960seconds, allexcept 5ofthereduce tasks are\\ncompleted. Howeverthese lastfewstragglers don't\\x02n-\\nishuntil 300seconds later.Theentire computation takes\\n1283 seconds, anincrease of44% inelapsed time.\\n5.5 Machine Failur es\\nInFigure 3(c),weshowanexecution ofthesortprogram\\nwhere weintentionally killed 200 outof1746 worker\\nprocesses several minutes into thecomputation. The\\nunderlying cluster scheduler immediately restarted new\\nworkerprocesses onthese machines (since only thepro-\\ncesses were killed, themachines were stillfunctioning\\nproperly).\\nThe workerdeaths showupasanegativeinput rate\\nsince some previously completed map workdisappears\\n(since thecorresponding map workerswere killed) and\\nneeds toberedone. There-execution ofthismap work\\nhappens relati velyquickly .The entire computation \\x02n-\\nishes in933seconds including startup overhead (just an\\nincrease of5%overthenormal execution time).\\n6Experience\\nWewrote the\\x02rst version oftheMapReduce library in\\nFebruary of2003, andmade signi\\x02cant enhancements to\\nitinAugust of2003, including thelocality optimization,\\ndynamic load balancing oftask execution across worker\\nmachines, etc.Since thattime, wehavebeen pleasantly\\nsurprised athowbroadly applicable theMapReduce li-\\nbrary hasbeen forthekinds ofproblems weworkon.\\nIthasbeen used across awide range ofdomains within\\nGoogle, including:\\n\\x0flarge-scale machine learning problems,\\n\\x0fclustering problems fortheGoogle Newsand\\nFroogle products,\\n\\x0fextraction ofdataused toproduce reports ofpopular\\nqueries (e.g. Google Zeitgeist),\\n\\x0fextraction ofproperties ofweb pages fornewexper-\\niments andproducts (e.g. extraction ofgeographi-\\ncallocations from alargecorpus ofweb pages for\\nlocalized search), and\\n\\x0flarge-scale graph computations.2003/032003/062003/092003/122004/032004/062004/0902004006008001000Number of instances in source tree\\nFigure 4:MapReduce instances overtime\\nNumber ofjobs 29,423\\nAverage jobcompletion time 634secs\\nMachine days used 79,186 days\\nInput data read 3,288 TB\\nIntermediate data produced 758TB\\nOutput data written 193TB\\nAverage workermachines perjob 157\\nAverage workerdeaths perjob 1.2\\nAverage map tasks perjob 3,351\\nAverage reduce tasks perjob 55\\nUnique map implementations 395\\nUnique reduce implementations 269\\nUnique map/r educe combinations 426\\nTable 1:MapReduce jobs runinAugust 2004\\nFigure 4showsthesigni\\x02cant growthinthenumber of\\nseparate MapReduce programs check edintoourprimary\\nsource code management system overtime, from 0in\\nearly 2003 toalmost 900 separate instances asoflate\\nSeptember 2004. MapReduce hasbeen sosuccessful be-\\ncause itmakesitpossible towrite asimple program and\\nrunitef\\x02ciently onathousand machines inthecourse\\nofhalfanhour,greatly speeding upthedevelopment and\\nprototyping cycle. Furthermore, itallowsprogrammers\\nwho havenoexperience with distrib uted and/or parallel\\nsystems toexploit largeamounts ofresources easily .\\nAttheendofeach job, theMapReduce library logs\\nstatistics about thecomputational resources used bythe\\njob.InTable 1,weshowsome statistics forasubset of\\nMapReduce jobs runatGoogle inAugust 2004.\\n6.1 Large-Scale Indexing\\nOne ofourmost signi\\x02cant uses ofMapReduce todate\\nhasbeen acomplete rewrite oftheproduction index-\\nToappear inOSDI 2004 10\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 10}, page_content='ingsystem thatproduces thedata structures used forthe\\nGoogle web search service. The indexing system takes\\nasinput alargesetofdocuments thathavebeen retrie ved\\nbyourcrawling system, stored asasetofGFS \\x02les. The\\nrawcontents forthese documents aremore than 20ter-\\nabytes ofdata. Theindexing process runs asasequence\\nof\\x02vetotenMapReduce operations. Using MapReduce\\n(instead ofthead-hoc distrib uted passes intheprior ver-\\nsion oftheindexing system) hasprovided severalbene-\\n\\x02ts:\\n\\x0fTheindexing code issimpler ,smaller ,andeasier to\\nunderstand, because thecode thatdeals with fault\\ntolerance, distrib ution andparallelization ishidden\\nwithin theMapReduce library .Forexample, the\\nsizeofonephase ofthecomputation dropped from\\napproximately 3800 lines ofC++ code toapprox-\\nimately 700 lines when expressed using MapRe-\\nduce.\\n\\x0fTheperformance oftheMapReduce library isgood\\nenough that wecankeepconceptually unrelated\\ncomputations separate, instead ofmixing them to-\\ngether toavoidextra passes overthedata. This\\nmakesiteasy tochange theindexing process. For\\nexample, one change that took afewmonths to\\nmakeinouroldindexing system took only afew\\ndays toimplement inthenewsystem.\\n\\x0fThe indexing process hasbecome much easier to\\noperate, because most oftheproblems caused by\\nmachine failures, slowmachines, andnetw orking\\nhiccups aredealt with automatically bytheMapRe-\\nduce library without operator interv ention. Further -\\nmore, itiseasy toimpro vetheperformance ofthe\\nindexing process byadding newmachines tothein-\\ndexing cluster .\\n7Related Work\\nManysystems haveprovided restricted programming\\nmodels andused therestrictions toparallelize thecom-\\nputation automatically .Forexample, anassociati vefunc-\\ntioncanbecomputed overallpre\\x02x esofanNelement\\narray inlogNtime onNprocessors using parallel pre\\x02x\\ncomputations [6,9,13].MapReduce canbeconsidered\\nasimpli\\x02cation anddistillation ofsome ofthese models\\nbased onourexperience with largereal-w orld compu-\\ntations. More signi\\x02cantly ,weprovide afault-tolerant\\nimplementation that scales tothousands ofprocessors.\\nIncontrast, most oftheparallel processing systems have\\nonly been implemented onsmaller scales andleavethe\\ndetails ofhandling machine failures totheprogrammer .\\nBulk Synchronous Programming [17]andsome MPI\\nprimiti ves[11]provide higher -levelabstractions thatmakeiteasier forprogrammers towrite parallel pro-\\ngrams. Akeydifference between these systems and\\nMapReduce isthatMapReduce exploits arestricted pro-\\ngramming model toparallelize theuser program auto-\\nmatically andtoprovide transparent fault-tolerance.\\nOur locality optimization drawsitsinspiration from\\ntechniques such asactivedisks [12,15],where compu-\\ntation ispushed into processing elements thatareclose\\ntolocal disks, toreduce theamount ofdata sent across\\nI/Osubsystems orthenetw ork. Werunoncommodity\\nprocessors towhich asmall number ofdisks aredirectly\\nconnected instead ofrunning directly ondisk controller\\nprocessors, butthegeneral approach issimilar .\\nOur backup task mechanism issimilar totheeager\\nscheduling mechanism emplo yedintheCharlotte Sys-\\ntem [3].One oftheshortcomings ofsimple eager\\nscheduling isthatifagiventaskcauses repeated failures,\\ntheentire computation failstocomplete. We\\x02xsome in-\\nstances ofthisproblem with ourmechanism forskipping\\nbadrecords.\\nTheMapReduce implementation relies onanin-house\\ncluster management system that isresponsible fordis-\\ntributing andrunning user tasks onalargecollection of\\nshared machines. Though notthefocus ofthispaper ,the\\ncluster management system issimilar inspirit toother\\nsystems such asCondor [16].\\nThe sorting facility thatisapart oftheMapReduce\\nlibrary issimilar inoperation toNOW-Sort [1].Source\\nmachines (map workers)partition thedata tobesorted\\nandsend ittooneofRreduce workers. Each reduce\\nworkersorts itsdata locally (inmemory ifpossible). Of\\ncourse NOW-Sort does nothavetheuser-de\\x02nable Map\\nandReduce functions thatmakeourlibrary widely appli-\\ncable.\\nRiver[2]provides aprogramming model where pro-'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 10}, page_content='course NOW-Sort does nothavetheuser-de\\x02nable Map\\nandReduce functions thatmakeourlibrary widely appli-\\ncable.\\nRiver[2]provides aprogramming model where pro-\\ncesses communicate with each other bysending data\\noverdistrib uted queues. LikeMapReduce, theRiver\\nsystem tries toprovide good average case performance\\neveninthepresence ofnon-uniformities introduced by\\nheterogeneous hardw areorsystem perturbations. River\\nachie vesthisbycareful scheduling ofdisk andnetw ork\\ntransfers toachie vebalanced completion times. MapRe-\\nduce hasadifferent approach. Byrestricting thepro-\\ngramming model, theMapReduce frame workisable\\ntopartition theproblem into alargenumber of\\x02ne-\\ngrained tasks. These tasks aredynamically scheduled\\nonavailable workerssothatfaster workersprocess more\\ntasks. The restricted programming model also allows\\nustoschedule redundant executions oftasks near the\\nendofthejobwhich greatly reduces completion time in\\nthepresence ofnon-uniformities (such assloworstuck\\nworkers).\\nBAD-FS [5]hasaverydifferent programming model\\nfrom MapReduce, andunlik eMapReduce, istargeted to\\nToappear inOSDI 2004 11'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 11}, page_content=\"theexecution ofjobs across awide-area netw ork. How-\\never,there aretwofundamental similarities. (1)Both\\nsystems useredundant execution torecoverfrom data\\nloss caused byfailures. (2)Both uselocality-a ware\\nscheduling toreduce theamount ofdata sentacross con-\\ngested netw orklinks.\\nTACC [7]isasystem designed tosimplify con-\\nstruction ofhighly-a vailable netw orkedservices. Like\\nMapReduce, itrelies onre-execution asamechanism for\\nimplementing fault-tolerance.\\n8Conclusions\\nTheMapReduce programming model hasbeen success-\\nfully used atGoogle formanydifferent purposes. We\\nattrib utethissuccess toseveralreasons. First, themodel\\niseasy touse,evenforprogrammers without experience\\nwith parallel anddistrib uted systems, since ithides the\\ndetails ofparallelization, fault-tolerance, locality opti-\\nmization, andload balancing. Second, alargevariety\\nofproblems areeasily expressible asMapReduce com-\\nputations. Forexample, MapReduce isused forthegen-\\neration ofdata forGoogle' sproduction web search ser-\\nvice, forsorting, fordata mining, formachine learning,\\nandmanyother systems. Third, wehavedeveloped an\\nimplementation ofMapReduce thatscales tolargeclus-\\ntersofmachines comprising thousands ofmachines. The\\nimplementation makesef\\x02cient useofthese machine re-\\nsources andtherefore issuitable foruseonmanyofthe\\nlargecomputational problems encountered atGoogle.\\nWehavelearned severalthings from thiswork. First,\\nrestricting theprogramming model makesiteasy topar-\\nallelize anddistrib utecomputations andtomakesuch\\ncomputations fault-tolerant. Second, netw orkbandwidth\\nisascarce resource. Anumber ofoptimizations inour\\nsystem aretherefore targeted atreducing theamount of\\ndatasentacross thenetw ork: thelocality optimization al-\\nlowsustoread data from local disks, andwriting asingle\\ncopyoftheintermediate data tolocal disk savesnetw ork\\nbandwidth. Third, redundant execution canbeused to\\nreduce theimpact ofslowmachines, andtohandle ma-\\nchine failures anddata loss.\\nAckno wledgements\\nJosh Levenber ghasbeen instrumental inrevising and\\nextending theuser-levelMapReduce API with anum-\\nberofnewfeatures based onhisexperience with using\\nMapReduce andother people' ssuggestions forenhance-\\nments. MapReduce reads itsinput from andwrites its\\noutput totheGoogle FileSystem [8].Wewould liketo\\nthank Mohit Aron, HowardGobiof f,Markus Gutschk e,DavidKramer ,Shun-T akLeung, andJosh Redstone for\\ntheir workindeveloping GFS. Wewould also liketo\\nthank PercyLiang andOlcan Sercinoglu fortheir work\\nindeveloping thecluster management system used by\\nMapReduce. MikeBurro ws,Wilson Hsieh, Josh Leven-\\nberg,Sharon Perl, Rob Pike,andDebby Wallach pro-\\nvided helpful comments onearlier drafts ofthis pa-\\nper.Theanon ymous OSDI reviewers, andourshepherd,\\nEric Brewer,provided manyuseful suggestions ofareas\\nwhere thepaper could beimpro ved.Finally ,wethank all\\ntheusers ofMapReduce within Google' sengineering or-\\nganization forproviding helpful feedback, suggestions,\\nandbugreports.\\nRefer ences\\n[1]Andrea C.Arpaci-Dusseau, Remzi H.Arpaci-Dusseau,\\nDavidE.Culler ,Joseph M.Hellerstein, andDavidA.Pat-\\nterson. High-performance sorting onnetw orks ofwork-\\nstations. InProceedings ofthe1997 ACMSIGMOD In-\\nternational Confer ence onMana gement ofData ,Tucson,\\nArizona, May 1997.\\n[2]Remzi H.Arpaci-Dusseau, Eric Anderson, Noah\\nTreuhaft, DavidE.Culler ,Joseph M.Hellerstein, David\\nPatterson, and Kathy Yelick. Cluster I/Owith River:\\nMaking thefastcase common. InProceedings oftheSixth\\nWorkshop onInput/Output inParallel and Distrib uted\\nSystems (IOP ADS '99),pages 10\\x9622, Atlanta, Geor gia,\\nMay 1999.\\n[3]Arash Baratloo, Mehmet Karaul, ZviKedem, andPeter\\nWyckoff.Charlotte: Metacomputing ontheweb.InPro-\\nceedings ofthe9thInternational Confer ence onParallel\\nandDistrib uted Computing Systems ,1996.\\n[4]Luiz A.Barroso, JeffreyDean, andUrsHolzle. Web\\nsearch foraplanet: TheGoogle cluster architecture. IEEE\\nMicr o,23(2):22\\x9628, April 2003.\\n[5]John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau,\"),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 11}, page_content='[4]Luiz A.Barroso, JeffreyDean, andUrsHolzle. Web\\nsearch foraplanet: TheGoogle cluster architecture. IEEE\\nMicr o,23(2):22\\x9628, April 2003.\\n[5]John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau,\\nRemzi H.Arpaci-Dusseau, andMiron Livny.Explicit\\ncontrol inabatch-a waredistrib uted \\x02lesystem. InPro-\\nceedings ofthe1stUSENIX Symposium onNetwork ed\\nSystems Design andImplementation NSDI ,March 2004.\\n[6]Guy E.Blelloch. Scans asprimiti veparallel operations.\\nIEEE Transactions onComputer s,C-38(11), November\\n1989.\\n[7]Armando Fox, StevenD.Gribble, Yatin Chawathe,\\nEric A.Brewer,andPaulGauthier .Cluster -based scal-\\nable netw orkservices. InProceedings ofthe16th ACM\\nSymposium onOper ating System Principles ,pages 78\\x96\\n91,Saint-Malo, France, 1997.\\n[8]Sanjay Ghema wat,HowardGobiof f,andShun-T akLe-\\nung. TheGoogle \\x02lesystem. In19th Symposium onOp-\\nerating Systems Principles ,pages 29\\x9643, LakeGeor ge,\\nNewYork,2003.\\nToappear inOSDI 2004 12'),\n",
       " Document(metadata={'source': 'pdf/mapreduce-osdi04.pdf', 'page': 12}, page_content='[9]S.Gorlatch. Systematic ef\\x02cient parallelization ofscan\\nandother listhomomorphisms. InL.Bouge, P.Fraigni-\\naud, A.Mignotte, andY.Robert, editors, Euro-Par\\'96.\\nParallel Processing ,Lecture Notes inComputer Science\\n1124, pages 401\\x96408. Springer -Verlag, 1996.\\n[10] Jim Gray . Sort benchmark home page.\\nhttp://research.microsoft.com/barc/SortBenchmark/.\\n[11] William Gropp, Ewing Lusk, and Anthon ySkjellum.\\nUsing MPI: Portable Parallel Programming with the\\nMessa ge-Passing Interface .MIT Press, Cambridge, MA,\\n1999.\\n[12] L.Huston, R.Sukthankar ,R.Wickremesinghe, M.Satya-\\nnarayanan, G.R.Ganger ,E.Riedel, andA.Ailamaki. Di-\\namond: Astorage architecture forearly discard ininter-\\nactivesearch. InProceedings ofthe2004 USENIX File\\nandStorageTechnolo gies FAST Confer ence,April 2004.\\n[13] Richard E.Ladner andMichael J.Fischer .Parallel pre\\x02x\\ncomputation. Journal oftheACM,27(4):831\\x96838, 1980.\\n[14] Michael O.Rabin. Ef\\x02cient dispersal ofinformation for\\nsecurity ,load balancing andfaulttolerance. Journal of\\ntheACM,36(2):335\\x96348, 1989.\\n[15] Erik Riedel, Christos Faloutsos, Garth A.Gibson, and\\nDavidNagle. Activedisks forlarge-scale data process-\\ning. IEEE Computer ,pages 68\\x9674, June 2001.\\n[16] Douglas Thain, Todd Tannenbaum, and Miron Livny.\\nDistrib uted computing inpractice: The Condor experi-\\nence. Concurr ency andComputation: Practice andEx-\\nperience ,2004.\\n[17] L.G.Valiant. Abridging model forparallel computation.\\nCommunications oftheACM,33(8):103\\x96111, 1997.\\n[18] Jim Wyllie. Spsort: Howtosort aterabyte quickly .\\nhttp://alme1.almaden.ibm.com/cs/spsort.pdf.\\nAWordFrequency\\nThis section contains aprogram thatcounts thenumber\\nofoccurrences ofeach unique wordinasetofinput \\x02les\\nspeci\\x02ed onthecommand line.\\n#include \"mapreduce/mapreduce.h\"\\n//User\\'smapfunction\\nclassWordCounter :publicMapper{\\npublic:\\nvirtual voidMap(const MapInput& input){\\nconststring& text=input.value();\\nconstintn=text.size();\\nfor(inti=0;i<n;){\\n//Skippastleading whitespace\\nwhile((i<n)&&isspace(text[i]))\\ni++;\\n//Findwordend\\nintstart=i;\\nwhile((i<n)&&!isspace(text[i]))\\ni++;if(start<i)\\nEmit(text.substr(start,i-start),\"1\");\\n}\\n}\\n};\\nREGISTER_MAPPER(WordCounter);\\n//User\\'sreducefunction\\nclassAdder:publicReducer {\\nvirtual voidReduce(ReduceInput* input){\\n//Iterate overallentries withthe\\n//samekeyandaddthevalues\\nint64value=0;\\nwhile(!input->done()) {\\nvalue+=StringToInt(input->value());\\ninput->NextValue();\\n}\\n//Emitsumforinput->key()\\nEmit(IntToString(value));\\n}\\n};\\nREGISTER_REDUCER(Adder);\\nintmain(int argc,char**argv){\\nParseCommandLineFlags(argc, argv);\\nMapReduceSpecification spec;\\n//Storelistofinputfilesinto\"spec\"\\nfor(inti=1;i<argc;i++){\\nMapReduceInput* input=spec.add_input();\\ninput->set_format(\"text\");\\ninput->set_filepattern(argv[i]);\\ninput->set_mapper_class(\"WordCounter\");\\n}\\n//Specify theoutputfiles:\\n///gfs/test/freq-00000-of-00100\\n///gfs/test/freq-00001-of-00100\\n//...\\nMapReduceOutput* out=spec.output();\\nout->set_filebase(\"/gfs/test/freq\");\\nout->set_num_tasks(100);\\nout->set_format(\"text\");\\nout->set_reducer_class(\"Adder\");\\n//Optional: dopartial sumswithinmap\\n//taskstosavenetwork bandwidth\\nout->set_combiner_class(\"Adder\");\\n//Tuningparameters: useatmost2000\\n//machines and100MBofmemorypertask\\nspec.set_machines(2000);\\nspec.set_map_megabytes(100);\\nspec.set_reduce_megabytes(100);\\n//Nowrunit\\nMapReduceResult result;\\nif(!MapReduce(spec, &result)) abort();\\n//Done:\\'result\\' structure contains info\\n//aboutcounters, timetaken,numberof\\n//machines used,etc.\\nreturn0;\\n}\\nToappear inOSDI 2004 13')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=\"text_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6h/1cvt_wdj6gn43zxvbgkd2jnh0000gn/T/ipykernel_6261/3596658761.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=\"text_index\", embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(persist_directory=\"text_index\", embedding_function=embeddings)\n",
    "\n",
    "retriver = db.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6h/1cvt_wdj6gn43zxvbgkd2jnh0000gn/T/ipykernel_6261/1148363849.py:1: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Resuma o que  MapReduce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A MapReduce  uma biblioteca de processamento paralelo de grande escala desenvolvida pela Google. Ela permite dividir grandes conjuntos de dados em partes menores, processar cada parte em paraleto em um grande nmero de mquinas (computadores) e ento combinar os resultados para obter a sada final.\\n\\nA MapReduce  composta por dois principais componentes:\\n\\n1. **Mapper**: Divide o dados em partes menores e processo cada parte em paraleto em uma ou mais mquinas.\\n2. **Reducer**: Combina os resultados dos Mappers em paraleto e obtm a sada final.\\n\\nA biblioteca MapReduce  muito flexvel e pode ser usada para realizar uma variedade de tarefas, como:\\n\\n* Procesamento de grandes conjuntos de dados\\n* Anlise de dados em larga escala\\n* Pre processamento de dados\\n* Agregao de dados\\n\\nA MapReduce  amplamente utilizada em muitas aplicaes, incluindo sistemas de armazenamento de dados, sistemas de recomendo e sistemas de busca.\\n\\nEm resumo, a MapReduce  uma ferramenta poderosa para processar grandes conjuntos de dados em paraleto, permitindo que os computadores trabalhem juntos para realizar tarefas complexas.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = retriver.invoke(question)\n",
    "answer = chain.invoke({\"input_documents\": context, \"question\": question})\n",
    "answer[\"output_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-langchain-IuScLoIt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
